{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOay4GueoTquxRKBgZgaahi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"3giHmUmOkP3x"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["### **How Does Categorical Cross-Entropy Work?**\n","\n","**Categorical Cross-Entropy** (CCE) is a loss function commonly used for multiclass classification tasks. It measures the difference between the predicted probability distribution (from the model) and the true distribution (represented by one-hot labels). The goal is to minimize this difference, driving the predicted probabilities closer to the true labels.\n","\n","---\n","\n","### **Mathematical Formula**\n","The categorical cross-entropy loss for a single sample is calculated as:\n","\n","$[\n","\\text{Loss} = -\\sum_{i=1}^C y_i \\cdot \\log(\\hat{y}_i)\n","]$\n","\n","Where:\n","- $( C )$ = number of classes.\n","- $( y_i )$ = true label for class \\( i \\) (1 if class \\( i \\) is the correct label, otherwise 0).\n","- $( \\hat{y}_i )$ = predicted probability for class \\( i \\) (output of the softmax function).\n","\n","---\n","\n","### **Step-by-Step Explanation**\n","\n","1. **True Label Representation**:\n","   - The true label \\( y \\) is usually represented as a **one-hot vector** for categorical cross-entropy.\n","   - Example for a 3-class problem:\n","     - True label: Class 2 â†’ One-hot vector: \\([0, 1, 0]\\).\n","\n","2. **Predicted Probabilities**:\n","   - The model's output for a multiclass classification is a vector of probabilities produced by the **softmax activation function**.\n","   - Example:\n","     - Predicted probabilities: \\([0.2, 0.7, 0.1]\\).\n","\n","3. **Loss Calculation**:\n","   - Categorical cross-entropy focuses on the **predicted probability of the correct class**.\n","   - Example for Class 2:\n","     $[\n","     \\text{Loss} = -\\left(0 \\cdot \\log(0.2) + 1 \\cdot \\log(0.7) + 0 \\cdot \\log(0.1)\\right)\n","     = -\\log(0.7)\n","     ]$\n","\n","4. **Overall Loss**:\n","   - For a batch of samples, the overall loss is the **mean categorical cross-entropy** across all samples.\n","\n","---\n","\n","### **Why Use Categorical Cross-Entropy?**\n","\n","1. **Focuses on the Correct Class**:\n","   - The one-hot label ensures that only the correct class contributes to the loss, as \\( y_i = 0 \\) for incorrect classes.\n","\n","2. **Probability-Based**:\n","   - The logarithmic term penalizes confident but incorrect predictions more heavily, encouraging the model to predict probabilities that are closer to the true distribution.\n","\n","3. **Softmax Compatibility**:\n","   - Works seamlessly with the output of the softmax function, which ensures the probabilities sum to 1.\n","\n","---\n","\n","### **Example in Python (with Keras)**\n","\n","```python\n","import numpy as np\n","from tensorflow.keras.losses import CategoricalCrossentropy\n","\n","# Example: True one-hot labels and predicted probabilities\n","y_true = np.array([[0, 1, 0]])  # Class 2\n","y_pred = np.array([[0.2, 0.7, 0.1]])  # Predicted probabilities\n","\n","# Calculate categorical cross-entropy loss\n","cce = CategoricalCrossentropy()\n","loss = cce(y_true, y_pred).numpy()\n","print(f\"Categorical Cross-Entropy Loss: {loss}\")\n","```\n","\n","---\n","\n","### **Key Intuitions**\n","\n","1. **Penalty for Low Confidence**:\n","   - If the model predicts low probability for the correct class, the log function returns a large negative value, leading to a high loss.\n","\n","2. **Encourages Confident Predictions**:\n","   - The model is rewarded (low loss) when it assigns a high probability to the correct class.\n","\n","---\n","\n","### **Comparison with Sparse Categorical Cross-Entropy**\n","\n","- **Categorical Cross-Entropy**: Requires labels in **one-hot encoded** format.\n","- **Sparse Categorical Cross-Entropy**: Accepts labels as integers (e.g., `0, 1, 2, ...`), simplifying preprocessing.\n","\n","---\n","\n"],"metadata":{"id":"fZg9CG_rkTlr"}},{"cell_type":"code","source":[],"metadata":{"id":"sefkcFvykV9c"},"execution_count":null,"outputs":[]}]}