{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMNMcAkDGt8FgmByLxWAMly"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### **Softmax vs. Sigmoid: Key Differences & When to Use Each**  \n","\n","Both **Softmax** and **Sigmoid** are activation functions used in classification problems, but they serve different purposes.  \n","\n","---\n","\n","## **1️ Sigmoid Activation Function**\n","$[\n","\\sigma(x) = \\frac{1}{1 + e^{-x}}\n","]$\n","- **Output range:** (0,1)  \n","- **Interpreted as probability** for each class **independently**.  \n","- **Threshold-based classification** (default **0.5** for binary classification).  \n","\n","###  **When to Use Sigmoid?**\n","| Scenario | Labels | Output Interpretation | Loss Function |\n","|----------|--------|----------------------|--------------|\n","| **Binary Classification** | `[0]` or `[1]` | Single probability (e.g., 0.8 → Class 1, 0.2 → Class 0) | `binary_crossentropy` |\n","| **Multi-Label Classification** (multiple 1s possible) | `[1, 0, 1, 0]` | Independent class probabilities | `binary_crossentropy` |\n","\n","###  **When NOT to Use Sigmoid?**\n","- **Multi-Class Classification (one-hot labels)** → **Use Softmax instead** because classes are mutually exclusive.  \n","\n","---\n","\n","## **2️ Softmax Activation Function**\n","$[\n","\\sigma(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n","]$\n","- **Output range:** (0,1), but **sum of all outputs = 1**.  \n","- Converts raw scores (logits) into a **probability distribution**.  \n","- **Highest probability class is the predicted class**.  \n","\n","###  **When to Use Softmax?**\n","| Scenario | Labels | Output Interpretation | Loss Function |\n","|----------|--------|----------------------|--------------|\n","| **Multi-Class Classification** (one-hot labels) | `[0, 0, 1, 0]` | Probabilities sum to 1 | `categorical_crossentropy` |\n","\n","###  **When NOT to Use Softmax?**\n","- **Binary classification** → Use **Sigmoid** instead.\n","- **Multi-label classification** → Use **Sigmoid** since labels are **not mutually exclusive**.\n","\n","---\n","\n","## **3️ Key Differences:**\n","| Feature | **Sigmoid** | **Softmax** |\n","|---------|------------|------------|\n","| **Output Range** | (0,1) | (0,1), but sum = 1 |\n","| **Use Case** | Binary & Multi-Label Classification | Multi-Class Classification |\n","| **Interpretation** | Independent probabilities for each class | Probability distribution over classes |\n","| **Threshold Needed?** | Yes (default 0.5) | No, highest probability is the predicted class |\n","| **Loss Function** | `binary_crossentropy` | `categorical_crossentropy` |\n","\n","---\n","\n","## **Summary: When to Use Each**\n","✔ **Binary Classification (Yes/No, 0/1)?** → **Use Sigmoid**  \n","✔ **Multi-Class (One-hot labels, pick one)?** → **Use Softmax**  \n","✔ **Multi-Label (Can belong to multiple classes)?** → **Use Sigmoid**  \n","\n","---\n","\n","## **4️ Example: Softmax vs. Sigmoid in Keras**\n","### **Binary Classification (Sigmoid)**\n","```python\n","model.add(Dense(1, activation='sigmoid'))  # 1 neuron, sigmoid activation\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","```\n","\n","### **Multi-Class Classification (Softmax)**\n","```python\n","model.add(Dense(10, activation='softmax'))  # 10 neurons for 10 classes\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","```\n","\n","---\n","\n","### **Final Takeaway:**\n","- **Softmax → Multi-Class (One-Hot Labels)**  \n","- **Sigmoid → Binary OR Multi-Label (Independent Labels)**  \n","- **NEVER use Sigmoid for One-Hot Encoded Multi-Class!**  \n"],"metadata":{"id":"t-e37TJBGjXv"}},{"cell_type":"code","source":[],"metadata":{"id":"ADDNhn0dGmmu"},"execution_count":null,"outputs":[]}]}