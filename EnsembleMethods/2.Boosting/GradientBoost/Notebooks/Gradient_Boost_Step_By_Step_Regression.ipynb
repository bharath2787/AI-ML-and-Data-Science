{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOVcZMoInwGD2x8LVF5yPTa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Znd7YMTZ0cGC"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["step-by-step through the **iterative process of Gradient Boosting for Regression** .\n","\n","---\n","\n","##  Iterative Process of Gradient Boosting (Regression with MSE)\n","\n","---\n","\n","###  **Step 0: Initialize with a constant prediction**\n","\n","We start by predicting a constant for all records ‚Äî usually the **mean** of the target variable $( y )$:\n","\n","$[\n","\\hat{y}^{(0)} = \\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i\n","]$\n","\n","This is our **initial model** ‚Äî the ‚Äúzeroth‚Äù iteration.\n","\n","---\n","\n","### üîÅ **Now, for each boosting round \\( m = 1, 2, ..., M \\):**\n","\n","---\n","\n","###  **Step 1: Compute the pseudo-residuals**\n","\n","Since we‚Äôre using **MSE**, the pseudo-residual is simply:\n","\n","$[\n","r_i^{(m)} = y_i - \\hat{y}_i^{(m-1)}\n","]$\n","\n","- $( r_i^{(m)} )$ tells us how wrong the previous model‚Äôs prediction was for point $( i )$.\n","- This is the **target** for the next regression tree.\n","\n","---\n","\n","###  **Step 2: Fit a regression tree**\n","\n","Train a small regression tree $( h_m(x) )$ on the residuals $( r_i^{(m)} )$ using the input features \\( x \\).\n","\n","- Each leaf outputs a **correction value** ‚Äî like ‚Äúhow much to tweak the prediction‚Äù in that region of the feature space.\n","\n","---\n","\n","###  **Step 3: Update the model**\n","\n","Now update your prediction as:\n","\n","$[\n","\\hat{y}_i^{(m)} = \\hat{y}_i^{(m-1)} + \\eta \\cdot h_m(x_i)\n","]$\n","\n","- $( \\eta )$ is the **learning rate** (typically between 0.01 and 0.3)\n","- This makes the update **conservative** and prevents overfitting\n","\n","---\n","\n","###  Repeat Steps 1‚Äì3 for M rounds\n","\n","Each time:\n","- Compute new residuals\n","- Fit new tree\n","- Add the new tree's output to the prediction\n","\n","---\n","\n","###  Final model:\n","\n","After $( M )$ rounds, your final prediction is:\n","\n","$[\n","\\hat{y}(x) = \\hat{y}^{(0)} + \\eta \\cdot \\sum_{m=1}^{M} h_m(x)\n","]$\n","\n","Each $( h_m(x) )$ is a tiny regression tree learning to fix the mistakes of the previous trees.\n","\n","---\n","\n","\n"],"metadata":{"id":"B7kfc1n40c16"}},{"cell_type":"code","source":[],"metadata":{"id":"hhwCywQu07NZ"},"execution_count":null,"outputs":[]}]}