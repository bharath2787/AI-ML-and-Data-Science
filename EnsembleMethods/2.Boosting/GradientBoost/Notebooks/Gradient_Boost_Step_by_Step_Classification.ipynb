{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPPlsxra2GsQWHBSCLdtx5C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"LP385a9s1VKN"},"outputs":[],"source":[]},{"cell_type":"markdown","source":[" **iterative process of Gradient Boosting for Binary Classification**, where the loss function is **log-loss** (also called logistic loss).\n","\n","---\n","\n","## Iterative Process of Gradient Boosting (Classification with Log-Loss)\n","\n","---\n","\n","###  **Step 0: Initialize with a constant prediction (log-odds)**\n","\n","We start by initializing the prediction with the same value for all records:\n","\n","$[\n","\\hat{y}^{(0)} = \\text{log-odds} = \\log\\left(\\frac{\\bar{p}}{1 - \\bar{p}}\\right)\n","\\quad \\text{where} \\quad \\bar{p} = \\frac{1}{n} \\sum_{i=1}^{n} y_i\n","]$\n","\n","This is the log-odds of the positive class, and it’s our initial prediction $( F_0(x) )$ (the raw score, not probability).\n","\n","---\n","\n","###  **Now, for each boosting round \\( m = 1, 2, ..., M \\):**\n","\n","---\n","\n","###  **Step 1: Convert log-odds to probabilities**\n","\n","Convert current raw predictions to probabilities using the **sigmoid function**:\n","\n","$[\n","p_i^{(m-1)} = \\frac{1}{1 + e^{-\\hat{y}_i^{(m-1)}}}\n","]$\n","\n","This gives us the model’s predicted probability for each sample.\n","\n","---\n","\n","###  **Step 2: Compute the pseudo-residuals**\n","\n","Now calculate the pseudo-residuals as the **negative gradient of log-loss**:\n","\n","$[\n","r_i^{(m)} = y_i - p_i^{(m-1)}\n","]$\n","\n","- If $( y_i = 1 )$ and $( p_i = 0.6 )$, then $( r_i = 0.4 )$\n","- This residual tells us how much correction is needed to get the model closer to the truth.\n","\n","---\n","\n","###  **Step 3: Fit a regression tree to residuals**\n","\n","Train a regression tree $( h_m(x) )$ on the pseudo-residuals $( r_i^{(m)} )$, using the input features $( x_i )$.\n","\n","- The tree tries to predict how much to change the **log-odds** for each region of the input space.\n","\n","---\n","\n","###  **Step 4: Update the model**\n","\n","Update predictions (still in log-odds space):\n","\n","$[\n","\\hat{y}_i^{(m)} = \\hat{y}_i^{(m-1)} + \\eta \\cdot h_m(x_i)\n","]$\n","\n","- $( \\eta )$ is the **learning rate**\n","- It slows down learning to prevent overfitting\n","\n","---\n","\n","###  Repeat Steps 1–4 for \\( M \\) rounds\n","\n","Each tree pushes the prediction a little closer to the correct class.\n","\n","---\n","\n","###  Final prediction:\n","\n","After M rounds, the final model is:\n","\n","$[\n","F(x) = \\hat{y}^{(0)} + \\eta \\cdot \\sum_{m=1}^{M} h_m(x)\n","]$\n","\n","Then, convert this raw score into probability using sigmoid:\n","\n","$[\n","p(x) = \\frac{1}{1 + e^{-F(x)}}\n","]$\n","\n","You can then threshold this (e.g., $( p > 0.5 )$) for classification.\n","\n","---\n","\n","###  Summary Table\n","\n","| Step | Description |\n","|------|-------------|\n","| 0 | Initialize with log-odds |\n","| 1 | Convert log-odds to probs (sigmoid) |\n","| 2 | Compute pseudo-residuals = \\( y - p \\) |\n","| 3 | Train regression tree on residuals |\n","| 4 | Update prediction (add scaled tree output) |\n","\n","---\n"],"metadata":{"id":"V_c-cVy61VjB"}},{"cell_type":"code","source":[],"metadata":{"id":"G0V0_t3a1xdd"},"execution_count":null,"outputs":[]}]}