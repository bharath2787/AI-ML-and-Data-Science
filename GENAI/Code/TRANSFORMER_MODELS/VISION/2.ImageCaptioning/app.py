# Import required libraries
import gradio as gr  # For building a simple web UI for the model
from transformers import BlipProcessor, BlipForConditionalGeneration  # BLIP model and its processor
from PIL import Image  # For image loading and manipulation
import torch  # PyTorch for model inference and tensor handling

# -----------------------------------------------
# STEP 1: Load the BLIP model and processor
# -----------------------------------------------

# Load the BLIP processor which handles preprocessing of images (resizing, normalization, etc.)
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")

# Load the pre-trained BLIP model for conditional image captioning
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

# -----------------------------------------------
# STEP 2: Define the function to generate captions
# -----------------------------------------------

def generate_caption(image: Image.Image) -> str:
    """
    Given an input image, generate a natural language caption.
    
    Args:
        image (PIL.Image): The input image uploaded by the user.
    
    Returns:
        str: A human-readable caption generated by the model.
    """
    # Convert image to RGB format to avoid issues with models expecting 3 color channels
    image = image.convert("RGB")
    
    # Preprocess the image and convert it into a format suitable for the model
    inputs = processor(images=image, return_tensors="pt")

    # Disable gradient computation (we're doing inference, not training)
    with torch.no_grad():
        # Generate output token IDs using the model
        out = model.generate(**inputs)

    # Decode the generated token IDs into a human-readable string (caption)
    caption = processor.decode(out[0], skip_special_tokens=True)
    
    return caption

# -----------------------------------------------
# STEP 3: Create a Gradio Interface
# -----------------------------------------------

# Create the Gradio UI for the image captioning app
demo = gr.Interface(
    fn=generate_caption,  # Function to call when the user uploads an image
    inputs=gr.Image(type="pil", label="Upload an image"),  # Input: an image in PIL format
    outputs=gr.Textbox(label="Generated Caption"),  # Output: a text box showing the caption
    title="Image Captioning with BLIP",  # App title
    description="Upload an image and get a natural language caption using the BLIP model from Hugging Face."  # Short description
)

# -----------------------------------------------
# STEP 4: Launch the App
# -----------------------------------------------

# Only run the app if this script is executed directly (good practice for modular code)
if __name__ == "__main__":
    demo.launch()  # Start the web server and show the UI
