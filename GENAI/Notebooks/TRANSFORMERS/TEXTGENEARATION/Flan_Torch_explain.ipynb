{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOuhSUD1Zekz3BLRDewOdzR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"nSiRd9LfNf52"},"outputs":[],"source":["# During **inference (generation)**, gradients are not needed."]},{"cell_type":"code","source":["    # # Generate response\n","    # with torch.no_grad():\n","    #     output_ids = model.generate(\n","    #         input_ids,\n","    #         max_length=150,\n","    #         num_beams=5,\n","    #         no_repeat_ngram_size=2,\n","    #         early_stopping=True\n","    #     )"],"metadata":{"id":"KDQ0aFeYNj-c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","### Model framework\n","\n","* **Flan-T5** is based on **T5**, which is implemented using **PyTorch** or **TensorFlow**.\n","* In the Hugging Face Transformers library, the default is **PyTorch**, so your model and inputs are PyTorch tensors.\n","\n","---\n","\n","### Why still use `torch.no_grad()` with Flan-T5?\n","\n","* Even though T5 is an encoder-decoder model (different architecture than GPT-2), it still uses PyTorch tensors.\n","* During **inference (generation)**, gradients are not needed.\n","* Wrapping the generation step in `with torch.no_grad():` improves **speed and reduces memory usage** by disabling gradient calculations.\n","\n","---\n","\n","### Summary for Flan-T5:\n","\n","* Your input/output are PyTorch tensors.\n","* You **should** use `torch.no_grad()` during generation to optimize inference.\n","\n","\n","---\n"],"metadata":{"id":"dlVgP-MnNkGr"}},{"cell_type":"code","source":[],"metadata":{"id":"ih3SU2dwNyY2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","### 1. **Beam search (`num_beams=5`)**\n","\n","* Beam search is a smarter way for the model to generate text.\n","* Instead of picking just one most likely word at each step, it keeps track of **multiple (5 here) best options (\"beams\")** simultaneously.\n","* It explores these options in parallel and chooses the sequence with the highest overall probability at the end.\n","* This helps produce **better, more coherent, and meaningful sentences** than just picking the single best word greedily.\n","\n","---\n","\n","### 2. **No repeat n-gram size (`no_repeat_ngram_size=2`)**\n","\n","* This prevents the model from repeating the **same sequence of words of length 2** (called a 2-gram) in the output.\n","* For example, it avoids phrases like:\n","  `\"I am am happy\"` or `\"the the dog\"`\n","* This helps make the generated text sound more natural and less repetitive.\n","\n","---\n","\n","### 3. **Early stopping (`early_stopping=True`)**\n","\n","* Normally, the model generates tokens until it reaches `max_length` or a special end token.\n","* With **early stopping enabled**, generation **stops as soon as the model is confident itâ€™s done**, before reaching the max length.\n","* This saves time and prevents unnecessarily long or awkward endings.\n","\n","---\n","\n","### Putting it all together:\n","\n","```python\n","output_ids = model.generate(\n","    input_ids,\n","    max_length=150,\n","    num_beams=5,             # Keep track of top 5 best sequences at each step\n","    no_repeat_ngram_size=2,  # Avoid repeating any 2-word sequences\n","    early_stopping=True      # Stop generating once a good answer is complete\n",")\n","```\n","\n","This setup helps generate **high-quality, coherent, and concise** text responses.\n","\n","---\n"],"metadata":{"id":"sPIPIbqMO0Za"}},{"cell_type":"code","source":[],"metadata":{"id":"hblzStOSO4Wy"},"execution_count":null,"outputs":[]}]}