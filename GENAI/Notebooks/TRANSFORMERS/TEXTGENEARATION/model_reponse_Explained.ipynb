{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPhTihHmXD84L22lPFZ/fMn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ujUaEs5JM0xW"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["\n","```python\n","response = tokenizer.decode(chat_history_ids[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n","```\n","\n","* `chat_history_ids` is the full sequence of token IDs generated so far — this includes **both the original input** you gave *and* the **new tokens the model just generated** as a response.\n","\n","* `input_ids.shape[-1]` is the length of your **original input tokens**.\n","\n","* `chat_history_ids[:, input_ids.shape[-1]:]` means:\n","  **Take all tokens *after* the input tokens** — basically, just the new generated tokens (the model’s reply).\n","\n","* `[0]` selects the first sequence in the batch (usually there's only one).\n","\n","* `tokenizer.decode(..., skip_special_tokens=True)` converts those token IDs back into human-readable text and removes any special tokens like \\`\\`.\n","\n","---\n","\n","### So in short:\n","\n","This line **extracts only the generated response tokens**, ignoring the original input, and turns them into readable text.\n","\n","---\n"],"metadata":{"id":"nMMCtLPnM31q"}},{"cell_type":"code","source":[],"metadata":{"id":"VAJvzbBdM6pf"},"execution_count":null,"outputs":[]}]}