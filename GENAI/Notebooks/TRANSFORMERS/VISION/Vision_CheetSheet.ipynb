{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO3JsJPYByINgqzHYgP6lXN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DYm9H94YAGjz"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["#Vision Cheet Sheet\n","\n","---\n","\n","##  Best Transformer Models for Common Vision Tasks\n","\n","| Vision Task                   | Recommended Model(s)                           | Hugging Face Model Hub                                                              | Notes                                |\n","| ----------------------------- | ---------------------------------------------- | ----------------------------------------------------------------------------------- | ------------------------------------ |\n","| **Image Classification**      | `ViT` (Vision Transformer), `ConvNeXt`, `DeiT` | `google/vit-base-patch16-224`, `facebook/deit-base-patch16-224`                     | Pretrained on ImageNet; plug & play  |\n","| **Object Detection**          | `DETR`, `DINO`, `YOLOS`                        | `facebook/detr-resnet-50`, `hustvl/yolos-small`                                     | Detects multiple objects in an image |\n","| **Image Captioning**          | `BLIP`, `ViT-GPT2`, `ClipCap`                  | `Salesforce/blip-image-captioning-base`, `nlpconnect/vit-gpt2-image-captioning`     | Converts image to text               |\n","| **Image Segmentation**        | `Mask2Former`, `SegFormer`, `DPT`              | `facebook/mask2former-swin-large-coco`, `nvidia/segformer-b5-finetuned-ade-640-640` | Classifies each pixel                |\n","| **Visual Question Answering** | `BLIP`, `OFA`, `VilBERT`                       | `Salesforce/blip-vqa-base`, `OFA-Sys/OFA-base`                                      | Answer questions about an image      |\n","| **Image-Text Retrieval**      | `CLIP`, `BLIP`                                 | `openai/clip-vit-base-patch32`, `Salesforce/blip-itm-base-coco`                     | Find best match image or text        |\n","| **Zero-Shot Classification**  | `CLIP`, `BLIP`                                 | `openai/clip-vit-base-patch32`                                                      | Classify images without training     |\n","| **Image Super-Resolution**    | `SwinIR`                                       | `caidas/swinir-classical-sr-x2-64`                                                  | Upscale low-res images               |\n","| **Depth Estimation**          | `DPT`, `LeRes`                                 | `Intel/dpt-hybrid-midas`, `intel-isl/MiDaS`                                         | Predict depth from image             |\n","\n","---\n","\n","##  Quick Notes:\n","\n","* Classify â†’ **ViT**, **DeiT**\n","* Detect objects â†’ **DETR**\n","* Caption images â†’ **BLIP**\n","* Segment objects/pixels â†’ **SegFormer**, **Mask2Former**\n","* Answer questions about images â†’ **BLIP**, **OFA**\n","* Match images to text (or vice versa) â†’ **CLIP**\n","* Upscale/enhance â†’ **SwinIR**\n","* Estimate depth from images â†’ **DPT**\n","\n","---\n","\n","##  How to Try These Models Easily\n","\n","```bash\n","pip install transformers timm torchvision gradio\n","```\n","\n","Then you can load them with:\n","\n","```python\n","from transformers import AutoProcessor, AutoModel\n","processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n","model = AutoModel.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n","```\n","\n","Or use `pipeline()` if it's supported:\n","\n","```python\n","from transformers import pipeline\n","pipe = pipeline(\"image-classification\", model=\"google/vit-base-patch16-224\")\n","```\n","\n","---\n","\n","##  Where to Explore More Models\n","\n","ðŸ‘‰ [Hugging Face Vision Models](https://huggingface.co/models?pipeline_tag=image-classification&library=transformers)\n","\n","---\n"],"metadata":{"id":"kNrw9qf3A3pd"}},{"cell_type":"code","source":[],"metadata":{"id":"ItS0HdFrBdN0"},"execution_count":null,"outputs":[]}]}