{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"mRT31SSxXV3h"},"source":["#### Load Dataset\n","\n","We are using **Quora's question pairs** dataset. The dataset has question pairs and label indicating whether the two questions are semantically same or not."]},{"cell_type":"code","metadata":{"id":"0lBv3dgEwqq4"},"source":["!wget http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hpC3heTZAjFS"},"source":["!ls -l"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BqQa6V_mxPmW"},"source":["import pandas as pd\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df = pd.read_csv('quora_duplicate_questions.tsv')"],"metadata":{"id":"zvb_ohXw_S8P"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FKRabOW2w6uK"},"source":["df = pd.read_csv('quora_duplicate_questions.tsv', sep='\\t')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KgCyTx3vxCX3"},"source":["df.sample(n=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.to_csv(\"test.csv\")"],"metadata":{"id":"FpnX63nU_wlv"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lSowDVrhzdF8"},"source":["df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TrZaXcds2EBv"},"source":["#Lets consider only 5000 records for demo here\n","small_df = df.sample(n=5000)\n","small_df.reset_index(inplace=True, drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y56WU_bI229B"},"source":["small_df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M8TUJPf228eB"},"source":["Check question pairs"]},{"cell_type":"code","metadata":{"id":"mLbCwPhW0sou"},"source":["idx = np.random.randint(0, small_df.shape[0])\n","\n","print('First Question:', small_df.loc[idx,'question1'])\n","print('Second Question:', small_df.loc[idx,'question2'])\n","print('Are questions duplicate?:', small_df.loc[idx,'is_duplicate'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MgyNoaxJzwJK"},"source":["#How many duplicate pairs\n","small_df.groupby(['is_duplicate']).count()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e90CNbdEYBmQ"},"source":["#### Text Preprocessing"]},{"cell_type":"code","metadata":{"id":"4Kwzaz3TYDms"},"source":["import re"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5SAbgW8c3eFp"},"source":["Function to clean up text. We can add more things here e.g Lemmatization"]},{"cell_type":"code","metadata":{"id":"ToHbUIpuYIqe"},"source":["def clean_str(text):\n","    #Using regex\n","    pattern = r'[^a-zA-z\\s]'\n","    text = re.sub(pattern, '', str(text))\n","    text = text.lower()\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3oS-vu6_3mag"},"source":["Apply the above function on question pairs"]},{"cell_type":"code","metadata":{"id":"bl0xKRCqYTku"},"source":["small_df['clean_question1'] = small_df['question1'].apply(clean_str)\n","small_df['clean_question2'] = small_df['question2'].apply(clean_str)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kiEgV6m9Y3pd"},"source":["small_df.sample(n=5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XeVGZlq63zRs"},"source":["Check question pairs with cleaned out text"]},{"cell_type":"code","metadata":{"id":"kmuTqKyJZEox"},"source":["idx = np.random.randint(0, small_df.shape[0])\n","print('First Question:', small_df.loc[idx,'clean_question1'])\n","print('Second Question:', small_df.loc[idx,'clean_question2'])\n","print('Are questions duplicate?:', small_df.loc[idx,'is_duplicate'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5kgwGmSrZYRO"},"source":["#### Vectorization"]},{"cell_type":"markdown","metadata":{"id":"0-AJ_OoE4TNU"},"source":["We are using TF-IDF vectorization here"]},{"cell_type":"code","metadata":{"id":"InAgkR8gZZ46"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mlzE7hMOZn9E"},"source":["vect = TfidfVectorizer(stop_words='english')\n","vect.fit(small_df['clean_question1'].tolist() + small_df['clean_question2'].tolist())\n","len(vect.get_feature_names_out())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WGPfp_qXZ9ll"},"source":["#### Check Similarity using TF-IDF"]},{"cell_type":"code","metadata":{"id":"Ez8kpvUfZ_f1"},"source":["#Convert question pairs in vector form\n","question1 = vect.transform(small_df['clean_question1'].tolist())\n","question2 = vect.transform(small_df['clean_question2'].tolist())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P4082RCoadkZ"},"source":["question1.shape, question2.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4ialqvlG4dM6"},"source":["We are using Cosine Similarity here to check similarity between two vectors. Other approaches can be Eucledean distance, Jaccard Index, Manhattan distance, WMD etc."]},{"cell_type":"code","metadata":{"id":"1mzmRUJEah4S"},"source":["from sklearn.metrics.pairwise import cosine_similarity"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZfzLP7q7bLQW"},"source":["idx = np.random.randint(0, small_df.shape[0])\n","\n","print('First Question:', small_df.loc[idx,'clean_question1'])\n","print('Second Question:', small_df.loc[idx,'clean_question2'])\n","print('Are questions duplicate?:', small_df.loc[idx,'is_duplicate'])\n","\n","print('Cosine Similarity:',cosine_similarity(question1[idx], question2[idx]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a-TCmTCb5DVo"},"source":["Function to calculate similarity based on a threshold"]},{"cell_type":"code","metadata":{"id":"KokMoairi7wl"},"source":["def check_similarity_tfidf(row, threshold=0.5):\n","\n","    similarity = cosine_similarity(vect.transform([str(row[6])]), vect.transform([str(row[7])]))\n","    if similarity >= threshold:\n","        return 1\n","    else:\n","        return 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n2oIvWinjtKO"},"source":["#Apply the function above\n","small_df['tfidf_similarity'] = small_df.apply(check_similarity_tfidf, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dMoz7-UB6sD5"},"source":["small_df.sample(n=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["small_df.to_csv(\"tdfidf.csv\")"],"metadata":{"id":"EDfSTAw_BUK_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wnws9Nym6T6s"},"source":["Calculate accuracy of this approach"]},{"cell_type":"code","source":["small_df['is_duplicate'] == small_df['tfidf_similarity']"],"metadata":{"id":"RLBwW0yBBqxv"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bJ4RcCV_6Wpf"},"source":["np.mean(small_df['is_duplicate'] == small_df['tfidf_similarity'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EWoeqK-Md5pu"},"source":["#### Check Similarity using Word2Vec embeddings"]},{"cell_type":"code","metadata":{"id":"bWd9VBy9d9mB"},"source":["import gensim.downloader as api"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Eihw0r1WeAg4"},"source":["#Load Glove model (similar to Word2Vec)\n","model = api.load('glove-wiki-gigaword-50')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t30WEKce8LsE"},"source":["#Model vocabulary\n","#model.index2word"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pdPiweLVgitZ"},"source":["def sentence2vec(model, sentence, embedding_size=50):\n","\n","    words = sentence.split()\n","\n","    #Initialize sentence vector with zeros\n","    sent2vec = np.zeros(embedding_size)\n","    sentence_length =0\n","\n","    for word in words:\n","\n","        if word in model.index2word:\n","            sent2vec = np.add(sent2vec, model[word])\n","            sentence_length += 1\n","\n","    #Average features (divide by sentence length)\n","    if sentence_length > 0:\n","        sent2vec = np.divide(sent2vec, sentence_length)\n","\n","    return np.expand_dims(sent2vec,axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-YrBX79R9KB4"},"source":["Find Cosine similarity between Question embeddings"]},{"cell_type":"code","metadata":{"id":"aDx7UFWJeavg"},"source":["idx = np.random.randint(0, small_df.shape[0])\n","print('First Question:', small_df.loc[idx,'clean_question1'])\n","print('Second Question:', small_df.loc[idx,'clean_question2'])\n","print('Are questions duplicate?:', small_df.loc[idx,'is_duplicate'])\n","\n","#Get Sentence embeddings\n","q1_embed = sentence2vec(model, small_df.loc[idx,'clean_question1'])\n","q2_embed = sentence2vec(model, small_df.loc[idx,'clean_question2'])\n","question_similarity = cosine_similarity(q1_embed, q2_embed)\n","print('Cosine Similarity:',question_similarity)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tCrekNS9-nU8"},"source":["#### Check Similarity using Word2Vec with SIF (Smooth Inverse Frequency)"]},{"cell_type":"code","metadata":{"id":"ieJmvLWN_lCL"},"source":["from collections import Counter\n","import itertools\n","from nltk.tokenize import word_tokenize"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1tZdiRhQFhRP"},"source":["import nltk\n","nltk.download('punkt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JlQ_518-F5IU"},"source":["Building word frequency map"]},{"cell_type":"code","metadata":{"id":"Q74RU-eQ_f0b"},"source":["def map_word_frequency(document):\n","    return Counter(itertools.chain(*document))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NPIgeigVDySK"},"source":["#Tokenize each question into word list\n","tokenized_question1 = [word_tokenize(row) for row in small_df['clean_question1'].tolist()]\n","tokenized_question2 = [word_tokenize(row) for row in small_df['clean_question2'].tolist()]\n","\n","#Build word count map\n","word_count_map = map_word_frequency(tokenized_question1 + tokenized_question2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CUUaSdGNFmPW"},"source":["word_count_map"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tfwNIwCw_qnG"},"source":["word_count_map['nickname']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eJzNK-Le-7kl"},"source":["def sentence2vec_sif(model, sentence, word_count_map, embedding_size=50):\n","\n","    words = sentence.split()\n","\n","    #Initialize sentence vector with zeros\n","    sent2vec = np.zeros(embedding_size)\n","    sentence_length =0\n","\n","    for word in words:\n","\n","        if word in model.index2word:\n","            #A word importance, high for less frequent words\n","            word_importance = 0.01 / (0.01 + word_count_map[word])\n","            word_embed = np.multiply(model[word], word_importance)\n","\n","            sent2vec = np.add(sent2vec, word_embed)\n","            sentence_length += 1\n","\n","    #Average features (divide by sentence length)\n","    if sentence_length > 0:\n","        sent2vec = np.divide(sent2vec, sentence_length)\n","\n","    return np.expand_dims(sent2vec,axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v3vVrYLpIrCy"},"source":["idx = np.random.randint(0, small_df.shape[0])\n","print('First Question:', small_df.loc[idx,'clean_question1'])\n","print('Second Question:', small_df.loc[idx,'clean_question2'])\n","print('Are questions duplicate?:', small_df.loc[idx,'is_duplicate'])\n","\n","#Get Sentence embeddings\n","q1_embed = sentence2vec_sif(model, small_df.loc[idx,'clean_question1'], word_count_map)\n","q2_embed = sentence2vec_sif(model, small_df.loc[idx,'clean_question2'], word_count_map)\n","question_similarity = cosine_similarity(q1_embed, q2_embed)\n","print('Cosine Similarity:',question_similarity)"],"execution_count":null,"outputs":[]}]}