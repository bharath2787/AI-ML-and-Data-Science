{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNM6y0OIRiBL8eBO5D8aeRS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zvx1cXeNLyAz"},"outputs":[],"source":["# use mistral 7b model from ollama for text generation"]},{"cell_type":"markdown","source":["---\n","\n","###  **Steps to Run LLMs Locally with Ollama and OpenWebUI**\n","\n","#### **1. Install Ollama**\n","\n","* Visit: [https://ollama.com](https://ollama.com)\n","* Download and install Ollama for your operating system (macOS, Windows, or Linux).\n","* After installation, verify it's working:\n","\n","  ```bash\n","  ollama --version\n","  ```\n","\n","#### **2. Pull a Model Using Ollama**\n","\n","* To download a model (e.g., `llama3`), use:\n","\n","  ```bash\n","  ollama pull llama3\n","  ```\n","* You can explore more models at [https://ollama.com/library](https://ollama.com/library)\n","\n","#### **3. Install Docker**\n","\n","* Download and install Docker from [https://www.docker.com/products/docker-desktop/](https://www.docker.com/products/docker-desktop/)\n","* Ensure Docker is running and accessible from the terminal:\n","\n","  ```bash\n","  docker --version\n","  ```\n","\n","#### **4. Run OpenWebUI via Docker**\n","\n","* Get the latest OpenWebUI Docker command from [https://github.com/open-webui/open-webui](https://github.com/open-webui/open-webui)\n","* Recommended Docker command:\n","\n","  ```bash\n","docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n","  ```\n","\n","#### **5. Access OpenWebUI in Your Browser**\n","\n","* Open your browser and go to:\n","\n","  ```\n","  http://localhost:3000\n","  ```\n","\n","#### **6. Sign In to OpenWebUI**\n","\n","* Register a new account or sign in with an existing one.\n","\n","#### **7. Start Using Downloaded Models**\n","\n","* On the top-left of the OpenWebUI interface, you'll see all the Ollama models you've downloaded.\n","* Click on any model to start chatting.\n","\n","---\n","\n","###  Tips\n","\n","* You can add new models at any time using `ollama pull <model-name>` and they'll auto-appear in OpenWebUI.\n","* OpenWebUI works with models hosted via Ollama – no extra configuration is needed if volumes are mounted correctly.\n","* For advanced config (e.g., custom system prompts or multiple users), refer to OpenWebUI’s GitHub documentation.\n","\n","---\n"],"metadata":{"id":"img5uKmqL3JC"}},{"cell_type":"code","source":[],"metadata":{"id":"rRnb34YuMLti"},"execution_count":null,"outputs":[]}]}