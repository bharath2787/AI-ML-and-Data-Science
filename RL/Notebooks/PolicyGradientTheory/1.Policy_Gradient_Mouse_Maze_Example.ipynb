{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM9LVElihPNUcWj3cHHxBMR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"oWVA6ZUGNEa6"},"outputs":[],"source":[]},{"cell_type":"code","source":[],"metadata":{"id":"DCubCKXZNKBD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let’s take a **concrete example** of a **mouse navigating a maze** to understand what inputs and outputs go into the **policy gradient neural network**, and how it learns from a few episodes.\n","\n","---\n","\n","##  Setup: Mouse Maze\n","\n","### Environment:\n","\n","* A **5x5 maze** grid.\n","* The **mouse starts at (0, 0)**.\n","* The **goal (cheese)** is at (4, 4).\n","* The mouse can take 4 actions: `up`, `down`, `left`, `right`.\n","\n","Each step gives:\n","\n","* -1 reward (for time penalty)\n","* +10 reward if it reaches the goal.\n","\n","---\n","\n","##  Neural Network (Policy)\n","\n","We have a neural network (NN) that **takes the current state** (position of the mouse) and **outputs probabilities for the 4 actions**.\n","\n","* **Input to NN**: current state (e.g., (0, 0), (1, 3), etc.)\n","\n","  * We'll use a 2D position as a vector, like `[x, y]`\n","* **Output of NN**: 4 numbers — probabilities for each action:\n","\n","  * $0.1, 0.6, 0.2, 0.1$ → (e.g., 60% chance to go down)\n","\n","---\n","\n","##  Let’s Simulate 3 Episodes\n","\n","We'll walk through:\n","\n","1. What the NN sees (input)\n","2. What it outputs (action probabilities)\n","3. What it selects (sampled action)\n","4. What reward it gets\n","5. What it sends to the gradient update\n","\n","---\n","\n","### Episode 1:\n","\n","#### Step 1:\n","\n","* **State**: \\[0, 0]\n","* **NN Output**: $0.1, 0.6, 0.2, 0.1$\n","* **Sampled Action**: `down` (index 1)\n","* **New State**: \\[1, 0], reward = -1\n","\n","#### Step 2:\n","\n","* **State**: \\[1, 0]\n","* **NN Output**: $0.2, 0.5, 0.2, 0.1$\n","* **Action**: `down` → \\[2, 0], reward = -1\n","\n","#### Step 3:\n","\n","* ...\n","* **Eventually** reaches goal at step 8\n","* **Total reward $R(\\tau_1)$**: +3 (because -1×7 + 10)\n","\n"," For each step:\n","\n","* Compute:\n","\n","  $$\n","  \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t)\n","  $$\n","* Multiply by $R(\\tau_1) = 3$\n","\n","---\n","\n","### Episode 2:\n","\n","* Different path.\n","* Reaches goal in 12 steps.\n","* $R(\\tau_2) = -2$\n","\n","Now the gradient update will **reduce** the probability of the actions it took.\n","\n","---\n","\n","### Episode 3:\n","\n","* Failed to reach goal in 15 steps.\n","* $R(\\tau_3) = -15$\n","\n","Bad episode → negative gradient will **discourage those actions**.\n","\n","---\n","\n","##  What Goes Into the Policy Gradient Update\n","\n","Let’s summarize it:\n","\n","### From each episode:\n","\n","We collect sequences of:\n","\n","| Time | State   | Action | log\\_prob(action) | Reward |\n","| ---- | ------- | ------ | ----------------- | ------ |\n","| 0    | \\[0, 0] | down   | -0.51             | -1     |\n","| 1    | \\[1, 0] | down   | -0.69             | -1     |\n","| ...  | ...     | ...    | ...               | ...    |\n","| 8    | \\[4, 4] | —      | —                 | +10    |\n","\n","Then compute:\n","\n","$$\n","\\text{Loss} = -\\sum_t \\log \\pi_\\theta(a_t|s_t) \\cdot R_t\n","$$\n","\n","This is the quantity the NN uses to **compute gradients and update its weights**.\n","\n","---\n","\n","##  input-output summary:\n","\n","| **Input to Neural Net**   | **Output from Neural Net**              |\n","| ------------------------- | --------------------------------------- |\n","| State = position `[x, y]` | Action probabilities `[p1, p2, p3, p4]` |\n","| Example: `[2, 1]`         | `[0.1, 0.7, 0.1, 0.1]`                  |\n","\n","This output is used to **sample an action**, and its log-probability is used to compute the gradient scaled by reward.\n","\n","---\n","\n","##  Repeat:\n","\n","After 3 episodes, we average the gradients and **update the weights** so that:\n","\n","* Good actions become more likely (positive reward)\n","* Bad actions become less likely (negative reward)\n","\n","---\n","\n"],"metadata":{"id":"wcHdELx5NKKf"}},{"cell_type":"code","source":[],"metadata":{"id":"L_Qj4gkWNKp9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"gat0x343NiJH"}},{"cell_type":"markdown","source":["##let's walk through it step by step in **Episode 1**, showing **what happens at each step**:\n","\n","---\n","\n","##  Goal:\n","\n","At **every step**, we:\n","\n","* Feed the current **state** into the policy network.\n","* Get the **probability distribution over actions**.\n","* **Sample an action** from that distribution.\n","* Take the action → get next state & reward.\n","* Store:\n","\n","  * $\\log \\pi_\\theta(a_t | s_t)$ → log probability of the taken action\n","  * $R_t$ → return/reward\n","\n","---\n","\n","##  Episode 1 — Suppose the Mouse Reaches Cheese in 8 Steps\n","\n","| Step | State   | NN Output (action probs) | Sampled Action | Next State | Reward |\n","| ---- | ------- | ------------------------ | -------------- | ---------- | ------ |\n","| 0    | \\[0, 0] | $0.1, 0.6, 0.2, 0.1$     | `down` (1)     | \\[1, 0]    | -1     |\n","| 1    | \\[1, 0] | $0.2, 0.5, 0.2, 0.1$     | `down` (1)     | \\[2, 0]    | -1     |\n","| 2    | \\[2, 0] | $0.25, 0.4, 0.25, 0.1$   | `right` (3)    | \\[2, 1]    | -1     |\n","| 3    | \\[2, 1] | $0.1, 0.2, 0.6, 0.1$     | `up` (0)       | \\[1, 1]    | -1     |\n","| 4    | \\[1, 1] | $0.1, 0.1, 0.1, 0.7$     | `right` (3)    | \\[1, 2]    | -1     |\n","| 5    | \\[1, 2] | $0.3, 0.2, 0.1, 0.4$     | `right` (3)    | \\[1, 3]    | -1     |\n","| 6    | \\[1, 3] | $0.2, 0.5, 0.1, 0.2$     | `down` (1)     | \\[2, 3]    | -1     |\n","| 7    | \\[2, 3] | $0.1, 0.1, 0.1, 0.7$     | `right` (3)    | \\[2, 4]    | +10    |\n","\n","---\n","\n","##  What Happens Internally at Each Step?\n","\n","At **each step $t$**:\n","\n","1. You pass state $s_t$ into the neural network.\n","2. Get softmax output → action probabilities: $\\pi_\\theta(a|s_t)$\n","3. Sample action $a_t$ from this distribution.\n","4. Compute $\\log \\pi_\\theta(a_t | s_t)$\n","5. Save it for training.\n","\n","Yes, **every step gives one probability per action**, but you only use the **log-prob of the sampled action** in the gradient:\n","\n","$$\n","\\text{Save: } \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t)\n","$$\n","\n","You don’t use all action probs — only the one you actually **sampled and took**.\n","\n","---\n","\n","##  Example:\n","\n","Let’s take Step 0 again:\n","\n","* State: \\[0, 0]\n","* NN output: $0.1, 0.6, 0.2, 0.1$\n","* Chosen action: `down` (index 1)\n","* So:\n","\n","  * $\\pi_\\theta(a_0 | s_0) = 0.6$\n","  * $\\log \\pi_\\theta(a_0 | s_0) = \\log(0.6) \\approx -0.51$\n","\n","This value is saved and **multiplied by reward later** in training.\n","\n","---\n","\n","##  After the Episode Ends\n","\n","We compute **returns** $R_t$ for each time step (total future reward from that step onward).\n","\n","For example (discount factor $\\gamma = 1$):\n","\n","| Step | Reward $r_t$ | Return $R_t$ |\n","| ---- | ------------ | ------------ |\n","| 0    | -1           | 3            |\n","| 1    | -1           | 4            |\n","| 2    | -1           | 5            |\n","| 3    | -1           | 6            |\n","| 4    | -1           | 7            |\n","| 5    | -1           | 8            |\n","| 6    | -1           | 9            |\n","| 7    | +10          | 10           |\n","\n","---\n","\n","##  Final Gradient Computation\n","\n","For each step:\n","\n","$$\n","\\text{Gradient} = \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot R_t\n","$$\n","\n","These gradients are **accumulated**, and we **take an average over the episode** to do the **parameter update**.\n","\n","---\n","\n","##  Summary\n","\n","* Yes, **every step** outputs a full action probability vector.\n","* We **only use the log-prob of the taken action**.\n","* That log-prob is scaled by the **total return** from that point onward.\n","* Gradient ascent makes **good actions more probable**, and **bad ones less**.\n","\n","---\n","\n"],"metadata":{"id":"xWzy0bS7NiL-"}},{"cell_type":"code","source":[],"metadata":{"id":"pQmm4glSN2o1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" **SUMMARY**\n","\n","\n","---\n","\n","##  What’s Actually Happening?\n","\n","At the **end of every episode**, the agent:\n","\n","* Looks back at **all the steps it took** (states, actions, rewards).\n","* Evaluates how **good or bad the entire episode was** based on **total reward or return**.\n","* Then uses that signal to **update the weights of the policy network**, so it:\n","\n","  * **Increases** the likelihood of the good actions.\n","  * **Decreases** the likelihood of the bad actions.\n","\n","---\n","\n","##  Per Episode vs Per Step\n","\n","* In **Policy Gradient (REINFORCE)**:\n","\n","  * The **policy network is updated once per episode**, not after every step.\n","  * All the gradients are **collected** during the episode.\n","  * At the end, the network uses the **final return or discounted returns** to evaluate all the actions taken.\n","\n",">the **final result** (or cumulative reward) decides whether the whole episode was **worth reinforcing or not**.\n","\n","---\n","\n"],"metadata":{"id":"6xO95niPOBOg"}},{"cell_type":"code","source":[],"metadata":{"id":"wyo2paNKOJfr"},"execution_count":null,"outputs":[]}]}