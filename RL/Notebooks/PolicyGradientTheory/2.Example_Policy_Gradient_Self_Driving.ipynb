{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMSYHsVQMV7E/t+mgF3TFLH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DSrVJHqAZAfb"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["**detailed, step-by-step walkthrough** of a **Policy Gradient agent in a self-driving car environment**, handling **continuous actions**.\n","\n","---\n","\n","#  Self-Driving Car Example: Policy Gradient (End-to-End)\n","\n","We’ll simulate one **full episode** (200 steps), and show exactly what happens at:\n","\n","1. **Input → Forward Pass → Action Sampling**\n","2. **Episode Collection**\n","3. **Loss Calculation**\n","4. **Weight Update (Gradient Ascent)**\n","\n","---\n","\n","##  STEP 1: Inputs & Action Space\n","\n","###  **State Input (observation)** — A vector from the car’s sensors\n","\n","```python\n","state = [\n","    0.7,    # speed (normalized 0 to 1)\n","    0.05,   # lane offset (−1.0 to +1.0)\n","    0.0,    # heading deviation (angle diff from lane center)\n","    0.6,    # distance to car ahead (0 to 1)\n","    0.1     # lateral velocity (side slip)\n","]  # shape: (5,)\n","```\n","\n","---\n","\n","###  **Action Output** — 3 continuous values:\n","\n","```python\n","action = [\n","    steering_angle_delta,   # range: -0.2 to +0.2\n","    throttle,               # range: 0.0 to 1.0\n","    brake                   # range: 0.0 to 1.0\n","]\n","```\n","\n","---\n","\n","##  STEP 2: Policy Network Forward Pass\n","\n","Assume the policy network outputs:\n","\n","* For each action dimension: a **μ (mean)** and **σ (std deviation)**\n","\n","So output = 6 values:\n","\n","```python\n","μ = [0.0, 0.9, 0.0]        # steer straight, high throttle, no brake\n","σ = [0.05, 0.1, 0.05]      # explore around these values\n","```\n","\n","We then sample:\n","\n","```python\n","action = [\n","  sample(Normal(0.0, 0.05)),    # steering\n","  sample(Normal(0.9, 0.1)),     # throttle\n","  sample(Normal(0.0, 0.05))     # brake\n","]\n","```\n","\n","Let’s say the sampled action was:\n","\n","```python\n","action = [0.01, 0.88, 0.02]\n","```\n","\n","---\n","\n","###  This forward pass + sampling is repeated for **200 steps**:\n","\n","For each of those steps:\n","\n","* Save:\n","\n","  ```python\n","  log_prob = log π(a_t | s_t)\n","  reward_t = environment response\n","  ```\n","\n","Store:\n","\n","```python\n","episode_log_probs = [log_prob_0, log_prob_1, ..., log_prob_199]\n","episode_rewards = [r_0, r_1, ..., r_199]\n","```\n","\n","---\n","\n","##  STEP 3: At the End of the Episode\n","\n","---\n","\n","###  A. Compute Discounted Returns $R_t$\n","\n","Use:\n","\n","$$\n","R_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots + \\gamma^{T-t} r_T\n","$$\n","\n","Example Python code:\n","\n","```python\n","def compute_returns(rewards, gamma=0.99):\n","    returns = []\n","    R = 0\n","    for r in reversed(rewards):\n","        R = r + gamma * R\n","        returns.insert(0, R)\n","    return returns\n","```\n","\n","Suppose after computing, you get:\n","\n","```python\n","returns = [4.2, 3.7, 3.0, ..., 0.1]  # 200 values\n","```\n","\n","---\n","\n","###  B. Sum Log Probs × Returns → Total Loss\n","\n","You already stored log probs:\n","\n","```python\n","log_probs = [-1.2, -0.7, -0.9, ..., -0.3]\n","```\n","\n","Then:\n","\n","```python\n","loss = 0\n","for log_prob, R in zip(log_probs, returns):\n","    loss += -log_prob * R\n","```\n","\n","This is your **policy gradient loss**:\n","\n","* **Actions with high reward** → log prob weighted positively → increase their probability.\n","* **Actions with low reward** → get suppressed.\n","\n","---\n","\n","##  C. Loss Backpropagation (Gradient Ascent)\n","\n","In PyTorch:\n","\n","```python\n","optimizer.zero_grad()\n","loss.backward()        # compute gradients\n","optimizer.step()       # update θ\n","```\n","\n","This:\n","\n","* Adjusts the weights in the neural network to **make good actions more likely**\n","* Over many episodes, the policy gets better at choosing the right steering, throttle, and brake.\n","\n","---\n","\n","##  Final Summary\n","\n","| Stage               | Details                                                                  |                    |\n","| ------------------- | ------------------------------------------------------------------------ | ------------------ |\n","| **Step (1–200)**    | Observe state → output μ, σ → sample action → store log\\_prob and reward |                    |\n","| **At episode end**  | Compute return $R_t$ for each step                                       |                    |\n","| **Loss function**   | $$( {Loss} = -\\sum\\_t \\log \\pi(a\\_t                                   | s\\_t) \\cdot R\\_t )$$ |\n","| **Backpropagation** | `loss.backward()` → gradients flow to update NN weights                  |                    |\n","| **Result**          | Policy improves: more confident in high-reward actions                   |                    |\n","\n","---\n","\n"],"metadata":{"id":"dDnNtNeWZFGv"}},{"cell_type":"code","source":[],"metadata":{"id":"tHCvhZVyZQaQ"},"execution_count":null,"outputs":[]}]}