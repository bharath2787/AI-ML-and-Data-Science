{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOablOYYJn6x6jfdtytI7iT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"GKLEeUE5atbB"},"outputs":[],"source":[]},{"cell_type":"markdown","source":[" The **γ (gamma)** parameter plays a **critical role** in reinforcement learning — it controls how far into the future the agent cares about rewards.\n","\n","---\n","\n","##  Gamma (γ): The Discount Factor\n","\n","$$\n","R_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots + \\gamma^{T-t} r_T\n","$$\n","\n","Gamma is a value between `0` and `1`:\n","\n","###  It **discounts** future rewards.\n","\n","* **High γ (close to 1)**:\n","  The agent **cares about long-term reward**.\n","  It will favor strategies that lead to higher total return, even if those returns come **later**.\n","\n","* **Low γ (close to 0)**:\n","  The agent is **short-sighted**.\n","  It only cares about **immediate rewards**, not long-term consequences.\n","\n","---\n","\n","##  Role in Policy Gradient\n","\n","In policy gradient loss:\n","\n","$$\n","\\text{Loss} = - \\sum_t \\log \\pi(a_t | s_t) \\cdot R_t\n","$$\n","\n","Where $R_t$ is the **discounted sum of future rewards**, and **γ controls that discount**.\n","\n","So:\n","\n","* If $\\gamma = 1$: You use the **raw sum** of all future rewards.\n","* If $\\gamma = 0.9$: You penalize rewards that come later (less certain, more delayed).\n","* If $\\gamma = 0$: Only the **immediate reward $r_t$** is used.\n","\n","---\n","\n","##  Intuition\n","\n","| γ Value | Agent Behavior  | Example                                   |\n","| ------- | --------------- | ----------------------------------------- |\n","| 0.0     | Myopic (greedy) | Brakes hard to avoid small penalty now    |\n","| 0.9     | Balanced view   | May slow down early to avoid future crash |\n","| 0.99    | Far-sighted     | Prefers safer but longer route to goal    |\n","\n","---\n","\n","##  Self-Driving Car Analogy\n","\n","* **Low γ (e.g. 0.1)**:\n","  The car might aggressively accelerate to get reward NOW, not realizing it might crash later.\n","\n","* **High γ (e.g. 0.99)**:\n","  The car may slow down early, stay centered, and take safe turns — because it “sees” that it will earn **higher cumulative reward** by being careful.\n","\n","---\n","\n","##  Summary\n","\n","| Parameter | Meaning                                         |\n","| --------- | ----------------------------------------------- |\n","| γ (gamma) | Discount factor for future rewards              |\n","| Range     | Between 0 and 1                                 |\n","| High γ    | More long-term planning                         |\n","| Low γ     | More short-term gain                            |\n","| In loss   | Affects $R_t$, which weights the log-prob terms |\n","\n","---\n"],"metadata":{"id":"zkFYvslUaxCf"}},{"cell_type":"code","source":[],"metadata":{"id":"vX9giM4Xa2J4"},"execution_count":null,"outputs":[]}]}