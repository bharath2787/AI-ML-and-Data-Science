{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM+KqXiwmp/gtiPiU8yA76p"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":[],"metadata":{"id":"y03WZ2RVguDY"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"igb_XE3GfWxQ"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["#Policy Gradient Theorem\n","It is a mathematical result that tells us how to compute the gradient of expected return with respect to the policy parameters in a reinforcement learning (RL) setting.\n","\n","This gradient is used to update the neural network (policy) so that it chooses better actions over time.\n","\n","---\n","\n","##  Equation Shown:\n","\n","$$\n","\\nabla_\\theta J(\\pi_\\theta) = \\mathbb{E} \\left[ \\sum_{t=0}^{\\infty} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot \\hat{V}_t \\right]\n","$$\n","\n","---\n","\n","##  What Each Term Means:\n","\n","| Part                                                    | Meaning                                                                                                                                         |                                                                                                                                                                    |\n","| ------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n","| **$\\nabla_\\theta J(\\pi_\\theta)$**                       | How to change the policy’s parameters (θ) to increase expected performance (J). This is the **gradient** that you’ll feed into backpropagation. |                                                                                                                                                                    |\n","| **$\\mathbb{E}$**                                        | Expectation (i.e. average) over many trajectories (episodes) — because reinforcement learning is stochastic.                                    |                                                                                                                                                                    |\n","| **$\\sum_{t=0}^\\infty$**                                 | Go through each time step $t$ in the episode (e.g., 200 steps).                                                                                 |                                                                                                                                                                    |\n","| \\*\\*( \\nabla\\_\\theta \\log \\pi\\_\\theta(a\\_t              | s\\_t) )\\*\\*                                                                                                                                     | This is the **log probability gradient**: tells us how to make the action $a_t$ more or less likely. It connects your action to the weights of the neural network. |\n","| **$\\hat{V}_t$**                                         | This is a placeholder for **how good action $a_t$ was**. It could be:                                                                           |                                                                                                                                                                    |\n","| → the total return $R_t$,                               |                                                                                                                                                 |                                                                                                                                                                    |\n","| → the advantage $A_t$,                                  |                                                                                                                                                 |                                                                                                                                                                    |\n","| → or any signal showing how valuable that decision was. |                                                                                                                                                 |                                                                                                                                                                    |\n","\n","---\n","\n","##  simple meaning\n","\n","| Annotation in Image                                   | Interpretation                                                                                |\n","| ----------------------------------------------------- | --------------------------------------------------------------------------------------------- |\n","| “how to change params(θ) to increase performance (J)” | This is the core goal — adjusting neural network weights to make the agent better.            |\n","| “average across all states and actions”               | The expectation ensures we're not just learning from one experience but **many**.             |\n","| “how to increase probability of action a”             | The log term with ∇ shows which direction to adjust weights to make good actions more likely. |\n","| “how good is action a”                                | That’s the reward or advantage — a **score** for how helpful an action was in hindsight.      |\n","\n","---\n","\n","##  In Simple Words:\n","\n","> We are nudging the neural network weights in a way that:\n",">\n","> * Makes **good actions more likely** in the future\n","> * Based on how **valuable** those actions turned out to be\n","> * Averaged over **many steps and episodes**\n","\n","---\n"],"metadata":{"id":"esAy25DYfYQf"}},{"cell_type":"code","source":[],"metadata":{"id":"wa_FnS7KfYvt"},"execution_count":null,"outputs":[]}]}