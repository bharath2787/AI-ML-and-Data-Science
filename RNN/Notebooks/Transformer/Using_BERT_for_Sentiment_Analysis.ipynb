{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOt2MxeeWjcoysikswsgugo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"2d4510f8c89e426bba4e6fbc01a7e4f7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fe684b2a686f4e1cb5edd8d525d167fb","IPY_MODEL_5a6bdf0ef45042fe83d3fdad6233a165","IPY_MODEL_1589649a8fcd48f49248963594468e2b"],"layout":"IPY_MODEL_c343e76fe0114561ad22f93b9264d376"}},"fe684b2a686f4e1cb5edd8d525d167fb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_29a0645713c14c7e8a4b0dfe6b9aee80","placeholder":"​","style":"IPY_MODEL_68a1fb9b4a8d4a0fa8dac4b401da0c31","value":"tokenizer_config.json: 100%"}},"5a6bdf0ef45042fe83d3fdad6233a165":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5fca7bac03ec409a91d27a11ad5128d6","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9a0b975854b547bbb9ef7dca638e71ea","value":48}},"1589649a8fcd48f49248963594468e2b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_123e8c2f486c4277a1ed74b22c42f3df","placeholder":"​","style":"IPY_MODEL_d96626881bca4141adb887ec95fbc7c1","value":" 48.0/48.0 [00:00&lt;00:00, 2.00kB/s]"}},"c343e76fe0114561ad22f93b9264d376":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29a0645713c14c7e8a4b0dfe6b9aee80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68a1fb9b4a8d4a0fa8dac4b401da0c31":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5fca7bac03ec409a91d27a11ad5128d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a0b975854b547bbb9ef7dca638e71ea":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"123e8c2f486c4277a1ed74b22c42f3df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d96626881bca4141adb887ec95fbc7c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"24d47dd2d9d44f35873464ef95fea59f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fc8e18a91e2d48439018c3e2d3ebedcd","IPY_MODEL_4383d7d64f094441b93bee5d4fe256ad","IPY_MODEL_dc67c2841b5d46eaa5a7cd776bc89014"],"layout":"IPY_MODEL_fcb1d7e7ea27441b9fa6cbc12bd143ec"}},"fc8e18a91e2d48439018c3e2d3ebedcd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5da4ff8c20e849998b7934f03ccd380a","placeholder":"​","style":"IPY_MODEL_6506bc541e534010b31ae4a0c852385a","value":"vocab.txt: 100%"}},"4383d7d64f094441b93bee5d4fe256ad":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_941a46414bee46e6b53e3c2b2735119f","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_519b294b95f94c7db4f88aa8fa27cff2","value":231508}},"dc67c2841b5d46eaa5a7cd776bc89014":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d025ece11a04783bae16e797e8c9572","placeholder":"​","style":"IPY_MODEL_202e9694e88b4f458dec9d651660fb5b","value":" 232k/232k [00:00&lt;00:00, 4.61MB/s]"}},"fcb1d7e7ea27441b9fa6cbc12bd143ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5da4ff8c20e849998b7934f03ccd380a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6506bc541e534010b31ae4a0c852385a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"941a46414bee46e6b53e3c2b2735119f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"519b294b95f94c7db4f88aa8fa27cff2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2d025ece11a04783bae16e797e8c9572":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"202e9694e88b4f458dec9d651660fb5b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"435ee8d7688a4f7e8adcd98e981d3278":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_21877033041644e59b4e6c1abeb50f05","IPY_MODEL_6b9772f54db84f56bd5f99ac742d438d","IPY_MODEL_40a3eeb54dd64a1a9651caeebff9cdf1"],"layout":"IPY_MODEL_dd86af1d660a47e58a3593db6ef79237"}},"21877033041644e59b4e6c1abeb50f05":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5b7125b03bb426aaba1fe6f3117619d","placeholder":"​","style":"IPY_MODEL_29ca9105fe8f4e768e9c6e3cc7fcad54","value":"tokenizer.json: 100%"}},"6b9772f54db84f56bd5f99ac742d438d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ebd524e873be49d7bdea8e46d1242720","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4fb2e02f52c04764a34515264db303a6","value":466062}},"40a3eeb54dd64a1a9651caeebff9cdf1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa096b62dc6c420f996a65bd9519773e","placeholder":"​","style":"IPY_MODEL_05ddea2d18fd4f7f83d016e49a0d260a","value":" 466k/466k [00:00&lt;00:00, 18.1MB/s]"}},"dd86af1d660a47e58a3593db6ef79237":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5b7125b03bb426aaba1fe6f3117619d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29ca9105fe8f4e768e9c6e3cc7fcad54":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ebd524e873be49d7bdea8e46d1242720":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fb2e02f52c04764a34515264db303a6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fa096b62dc6c420f996a65bd9519773e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05ddea2d18fd4f7f83d016e49a0d260a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b6772e005ba44b3d975970fa525c1e9d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a29353b2f6004b898e4e21a46224eb79","IPY_MODEL_a5d5b8d0b2a94424ae8b3aa419f974e4","IPY_MODEL_e607ebee2bcc43c5bd1eacd09aa66ab2"],"layout":"IPY_MODEL_6094c4607aa947e8888262d7764f0e46"}},"a29353b2f6004b898e4e21a46224eb79":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_242fa207ed25452a8cde94986a9511cf","placeholder":"​","style":"IPY_MODEL_24e5e926b9e44467a1f092c072ecb7a9","value":"config.json: 100%"}},"a5d5b8d0b2a94424ae8b3aa419f974e4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5d5392087ca4b38866b2c68e0ca58aa","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7df72811fb134cc8b8c62144e232b061","value":570}},"e607ebee2bcc43c5bd1eacd09aa66ab2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_456b4660684f42e7a726799b85b68f24","placeholder":"​","style":"IPY_MODEL_4d87c4b90adc4d768cd15ca7deab525b","value":" 570/570 [00:00&lt;00:00, 21.2kB/s]"}},"6094c4607aa947e8888262d7764f0e46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"242fa207ed25452a8cde94986a9511cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24e5e926b9e44467a1f092c072ecb7a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5d5392087ca4b38866b2c68e0ca58aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7df72811fb134cc8b8c62144e232b061":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"456b4660684f42e7a726799b85b68f24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d87c4b90adc4d768cd15ca7deab525b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["###The transformers library is from Hugging Face, and it provides:\n","\n","*Pretrained transformer models (like BERT, GPT, T5)\n","\n","*Pipelines for tasks like text classification, translation, summarization, etc."],"metadata":{"id":"Ifbd6y7UIFh_"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"6UwmHIfOzhPL","executionInfo":{"status":"ok","timestamp":1748399325322,"user_tz":-330,"elapsed":11500,"user":{"displayName":"Mukesh Kumar","userId":"00664345223903384782"}}},"outputs":[],"source":["!pip3 install --quiet transformers"]},{"cell_type":"code","source":["import tensorflow as tf"],"metadata":{"id":"en1plDKezytd","executionInfo":{"status":"ok","timestamp":1748399339106,"user_tz":-330,"elapsed":13774,"user":{"displayName":"Mukesh Kumar","userId":"00664345223903384782"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z8fDv4G00Puj","executionInfo":{"status":"ok","timestamp":1748399364042,"user_tz":-330,"elapsed":24934,"user":{"displayName":"Mukesh Kumar","userId":"00664345223903384782"}},"outputId":"98ddc27e-d646-4798-893f-99340d733e85"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["data_path = '/content/drive/MyDrive/0.Latest_DS_Course/RNN/Data/labeledTrainData.tsv'\n"],"metadata":{"id":"xybR6wXEE77I","executionInfo":{"status":"ok","timestamp":1748399366825,"user_tz":-330,"elapsed":16,"user":{"displayName":"Mukesh Kumar","userId":"00664345223903384782"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["##Data Preprocessing"],"metadata":{"id":"Z8iVTJGe0QA8"}},{"cell_type":"markdown","source":["#### Load Data"],"metadata":{"id":"P9BIJA8S0SG6"}},{"cell_type":"code","source":["!pip install sentencepiece\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"86a0Gk5A0RcF","executionInfo":{"status":"ok","timestamp":1748399390843,"user_tz":-330,"elapsed":15952,"user":{"displayName":"Mukesh Kumar","userId":"00664345223903384782"}},"outputId":"6a557a5f-b1ef-4d57-ae30-8295cb3aa1b0"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"]}]},{"cell_type":"markdown","source":["###What is sentencepiece?\n","* sentencepiece is a tokenizer developed by Google.\n","\n","* It is often used with models like T5, mBART, XLM-R, and ALBERT that don’t rely on word-based tokenization but instead use subword units (like byte-pair encoding or unigram models).\n","\n","* It allows language-independent tokenization — even for text without spaces, like Japanese or Chinese."],"metadata":{"id":"h2gqui7GIX_4"}},{"cell_type":"code","source":["import pandas as pd\n"],"metadata":{"id":"nHQetkeq0Xqa","executionInfo":{"status":"ok","timestamp":1748399390873,"user_tz":-330,"elapsed":24,"user":{"displayName":"Mukesh Kumar","userId":"00664345223903384782"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","#change file path to point to where you have stored the zip file.\n","df = pd.read_csv(data_path, header=0, delimiter=\"\\t\", quoting=3)\n"],"metadata":{"id":"v_sP_Uk00Ylr","executionInfo":{"status":"ok","timestamp":1748399395295,"user_tz":-330,"elapsed":4421,"user":{"displayName":"Mukesh Kumar","userId":"00664345223903384782"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["df.sample(n=5)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"a1aUuqFv0Z3L","executionInfo":{"status":"ok","timestamp":1748399395481,"user_tz":-330,"elapsed":180,"user":{"displayName":"Mukesh Kumar","userId":"00664345223903384782"}},"outputId":"01cf46b7-cff9-41db-cb6f-ee1bf49711cc"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["              id  sentiment                                             review\n","24642    \"575_3\"          0  \"This movie isn't very good. It's boring, and ...\n","14218  \"6471_10\"          1  \"An OUR GANG Comedy Short.<br /><br />The Gang...\n","589    \"11641_1\"          0  \"Although I am generally a proponent of the we...\n","11303   \"5617_8\"          1  \"After his classic film noir homage Chinatown ...\n","12169   \"2438_9\"          1  \"This was a gem. Amazing acting from the leads..."],"text/html":["\n","  <div id=\"df-516b1ca7-7f9e-4122-b15b-66f2be14465a\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>sentiment</th>\n","      <th>review</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>24642</th>\n","      <td>\"575_3\"</td>\n","      <td>0</td>\n","      <td>\"This movie isn't very good. It's boring, and ...</td>\n","    </tr>\n","    <tr>\n","      <th>14218</th>\n","      <td>\"6471_10\"</td>\n","      <td>1</td>\n","      <td>\"An OUR GANG Comedy Short.&lt;br /&gt;&lt;br /&gt;The Gang...</td>\n","    </tr>\n","    <tr>\n","      <th>589</th>\n","      <td>\"11641_1\"</td>\n","      <td>0</td>\n","      <td>\"Although I am generally a proponent of the we...</td>\n","    </tr>\n","    <tr>\n","      <th>11303</th>\n","      <td>\"5617_8\"</td>\n","      <td>1</td>\n","      <td>\"After his classic film noir homage Chinatown ...</td>\n","    </tr>\n","    <tr>\n","      <th>12169</th>\n","      <td>\"2438_9\"</td>\n","      <td>1</td>\n","      <td>\"This was a gem. Amazing acting from the leads...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-516b1ca7-7f9e-4122-b15b-66f2be14465a')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-516b1ca7-7f9e-4122-b15b-66f2be14465a button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-516b1ca7-7f9e-4122-b15b-66f2be14465a');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-2a1e9a98-27e0-40be-bc2a-bb23dd133a4c\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2a1e9a98-27e0-40be-bc2a-bb23dd133a4c')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-2a1e9a98-27e0-40be-bc2a-bb23dd133a4c button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\\"6471_10\\\"\",\n          \"\\\"2438_9\\\"\",\n          \"\\\"11641_1\\\"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\\"An OUR GANG Comedy Short.<br /><br />The Gang coerces Spanky into watching their younger siblings. Caring for these FORGOTTEN BABIES turns out to be quite a chore, leaving the little nipper with no choice but to come up with some ingenious solutions to the baby-sitting problem...<br /><br />Spanky is in his glory in this hilarious little film, arguably his best. Highlight: Spanky's retelling the plot of the TARZAN movie he's recently seen to the audience of infants. Movie mavens will recognize Billy Gilbert's voice in the radio drama.\\\"\",\n          \"\\\"This was a gem. Amazing acting from the leads Liam Cunningham, Orla Brady and all the supporting cast. The movie raises a subject not only pertinent to Ireland and Irish history but to many communities around the world and many marriage units within those communities. With intensity and sincerity the movie shows how the religious convictions and traditions drove a wedge on a loving and passionate family. The title \\\\\\\"Love divided\\\\\\\" couldn't capture it any better. Even though it was a true story and happening in Ireland of the 50th seeing how the life of the whole village erodes and \\\\\\\"pogroms\\\\\\\" are starting reminded me of Russian history. The intolerance and prejudice are still too powerful in the world and unfortunately it's deeply hidden inside the human nature. Just like in the movie the Liam Cunningham's character says \\\\\\\"the hatred had always been there under the surface\\\\\\\". It was interesting to watch the moral choices people were making in this story. Also the character of a catholic priest and what happened to him in the end of the story was quite meaningful. The story however gives hope that love of two people can conquer everything and love makes us better, stronger. Liam Cunningham's character goes through the whole transformation in the course of the story becoming a man he always wanted to be. Again acting is a top notch. Story is fast-paced. Irish countryside is as beautiful as ever. Highly recommended.\\\"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# Sentences and labels\n","sentences = df.review.values\n","labels = df.sentiment.values\n"],"metadata":{"id":"dH3ivMxJ0bEr","executionInfo":{"status":"ok","timestamp":1748399410247,"user_tz":-330,"elapsed":43,"user":{"displayName":"Mukesh Kumar","userId":"00664345223903384782"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## Tokenize Data using BERT Tokenizer"],"metadata":{"id":"p3gBHkil0jRM"}},{"cell_type":"code","source":["from transformers import *\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dtl4Srty0cm8","executionInfo":{"status":"ok","timestamp":1748399527053,"user_tz":-330,"elapsed":111048,"user":{"displayName":"Mukesh Kumar","userId":"00664345223903384782"}},"outputId":"5e4ca07a-3743-4c2f-f056-53ffdb238082"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n","GroupViT models are not usable since `tensorflow_probability` can't be loaded. It seems you have `tensorflow_probability` installed with the wrong tensorflow version.Please try to reinstall it following the instructions here: https://github.com/tensorflow/probability.\n","TAPAS models are not usable since `tensorflow_probability` can't be loaded. It seems you have `tensorflow_probability` installed with the wrong tensorflow version. Please try to reinstall it following the instructions here: https://github.com/tensorflow/probability.\n"]}]},{"cell_type":"code","source":["# Get BertTokenizer\n","# Loads a pretrained BERT tokenizer that transforms text into numerical tokens, making it ready to be fed into the BERT model.\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":845,"referenced_widgets":["2d4510f8c89e426bba4e6fbc01a7e4f7","fe684b2a686f4e1cb5edd8d525d167fb","5a6bdf0ef45042fe83d3fdad6233a165","1589649a8fcd48f49248963594468e2b","c343e76fe0114561ad22f93b9264d376","29a0645713c14c7e8a4b0dfe6b9aee80","68a1fb9b4a8d4a0fa8dac4b401da0c31","5fca7bac03ec409a91d27a11ad5128d6","9a0b975854b547bbb9ef7dca638e71ea","123e8c2f486c4277a1ed74b22c42f3df","d96626881bca4141adb887ec95fbc7c1","24d47dd2d9d44f35873464ef95fea59f","fc8e18a91e2d48439018c3e2d3ebedcd","4383d7d64f094441b93bee5d4fe256ad","dc67c2841b5d46eaa5a7cd776bc89014","fcb1d7e7ea27441b9fa6cbc12bd143ec","5da4ff8c20e849998b7934f03ccd380a","6506bc541e534010b31ae4a0c852385a","941a46414bee46e6b53e3c2b2735119f","519b294b95f94c7db4f88aa8fa27cff2","2d025ece11a04783bae16e797e8c9572","202e9694e88b4f458dec9d651660fb5b","435ee8d7688a4f7e8adcd98e981d3278","21877033041644e59b4e6c1abeb50f05","6b9772f54db84f56bd5f99ac742d438d","40a3eeb54dd64a1a9651caeebff9cdf1","dd86af1d660a47e58a3593db6ef79237","d5b7125b03bb426aaba1fe6f3117619d","29ca9105fe8f4e768e9c6e3cc7fcad54","ebd524e873be49d7bdea8e46d1242720","4fb2e02f52c04764a34515264db303a6","fa096b62dc6c420f996a65bd9519773e","05ddea2d18fd4f7f83d016e49a0d260a","b6772e005ba44b3d975970fa525c1e9d","a29353b2f6004b898e4e21a46224eb79","a5d5b8d0b2a94424ae8b3aa419f974e4","e607ebee2bcc43c5bd1eacd09aa66ab2","6094c4607aa947e8888262d7764f0e46","242fa207ed25452a8cde94986a9511cf","24e5e926b9e44467a1f092c072ecb7a9","a5d5392087ca4b38866b2c68e0ca58aa","7df72811fb134cc8b8c62144e232b061","456b4660684f42e7a726799b85b68f24","4d87c4b90adc4d768cd15ca7deab525b"]},"id":"J_OCm4WZ0gjb","executionInfo":{"status":"ok","timestamp":1748399614216,"user_tz":-330,"elapsed":1876,"user":{"displayName":"Mukesh Kumar","userId":"00664345223903384782"}},"outputId":"fa2fe7e0-e9b8-428b-c4ce-35fd77666447"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d4510f8c89e426bba4e6fbc01a7e4f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24d47dd2d9d44f35873464ef95fea59f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"435ee8d7688a4f7e8adcd98e981d3278"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n","loading file chat_template.jinja from cache at None\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6772e005ba44b3d975970fa525c1e9d"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.52.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n"]}]},{"cell_type":"markdown","source":["###.from_pretrained('bert-base-uncased')\n","###This function:\n","\n","* Downloads the vocabulary and configuration of the BERT tokenizer that was used with the 'bert-base-uncased' model.\n","\n","* 'bert-base-uncased' is a popular variant of BERT that:\n","\n","  - Has 12 layers and 110 million parameters.\n","\n","  - Converts all text to lowercase (hence \"uncased\").\n","\n","  - Is trained on English Wikipedia + BooksCorpus."],"metadata":{"id":"vk26DtpjHmZP"}},{"cell_type":"code","source":["tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n"],"metadata":{"id":"t_9nFNAG0hzr","executionInfo":{"status":"ok","timestamp":1748399730219,"user_tz":-330,"elapsed":112838,"user":{"displayName":"Mukesh Kumar","userId":"00664345223903384782"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["sentences[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":123},"id":"O6B1RPXK0ikb","executionInfo":{"status":"ok","timestamp":1748357920287,"user_tz":-330,"elapsed":19,"user":{"displayName":"Mukesh Kumar","userId":"00664345223903384782"}},"outputId":"e71e6d79-13de-4699-ab24-61a31600ffaf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\"With all this stuff going down at the moment with MJ i\\'ve started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ\\'s feeling towards the press and also the obvious message of drugs are bad m\\'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci\\'s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ\\'s music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ\\'s bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i\\'ve gave this subject....hmmm well i don\\'t know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["#Check tokenized text\n","print(tokenized_texts[0])\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H3ult9VdCM44","executionInfo":{"status":"ok","timestamp":1748357920287,"user_tz":-330,"elapsed":10,"user":{"displayName":"Mukesh Kumar","userId":"00664345223903384782"}},"outputId":"7e486775-1ac1-46d6-ca28-72d84f66e363"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['\"', 'with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'm', '##j', 'i', \"'\", 've', 'started', 'listening', 'to', 'his', 'music', ',', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', ',', 'watched', 'the', 'wi', '##z', 'and', 'watched', 'moon', '##walker', 'again', '.', 'maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent', '.', 'moon', '##walker', 'is', 'part', 'biography', ',', 'part', 'feature', 'film', 'which', 'i', 'remember', 'going', 'to', 'see', 'at', 'the', 'cinema', 'when', 'it', 'was', 'originally', 'released', '.', 'some', 'of', 'it', 'has', 'subtle', 'messages', 'about', 'm', '##j', \"'\", 's', 'feeling', 'towards', 'the', 'press', 'and', 'also', 'the', 'obvious', 'message', 'of', 'drugs', 'are', 'bad', 'm', \"'\", 'kay', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'visually', 'impressive', 'but', 'of', 'course', 'this', 'is', 'all', 'about', 'michael', 'jackson', 'so', 'unless', 'you', 'remotely', 'like', 'm', '##j', 'in', 'anyway', 'then', 'you', 'are', 'going', 'to', 'hate', 'this', 'and', 'find', 'it', 'boring', '.', 'some', 'may', 'call', 'm', '##j', 'an', 'ego', '##tist', 'for', 'consent', '##ing', 'to', 'the', 'making', 'of', 'this', 'movie', 'but', 'm', '##j', 'and', 'most', 'of', 'his', 'fans', 'would', 'say', 'that', 'he', 'made', 'it', 'for', 'the', 'fans', 'which', 'if', 'true', 'is', 'really', 'nice', 'of', 'him', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'the', 'actual', 'feature', 'film', 'bit', 'when', 'it', 'finally', 'starts', 'is', 'only', 'on', 'for', '20', 'minutes', 'or', 'so', 'excluding', 'the', 'smooth', 'criminal', 'sequence', 'and', 'joe', 'pe', '##sc', '##i', 'is', 'convincing', 'as', 'a', 'psycho', '##pathic', 'all', 'powerful', 'drug', 'lord', '.', 'why', 'he', 'wants', 'm', '##j', 'dead', 'so', 'bad', 'is', 'beyond', 'me', '.', 'because', 'm', '##j', 'overheard', 'his', 'plans', '?', 'nah', ',', 'joe', 'pe', '##sc', '##i', \"'\", 's', 'character', 'ran', '##ted', 'that', 'he', 'wanted', 'people', 'to', 'know', 'it', 'is', 'he', 'who', 'is', 'supplying', 'drugs', 'etc', 'so', 'i', 'dunn', '##o', ',', 'maybe', 'he', 'just', 'hates', 'm', '##j', \"'\", 's', 'music', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'lots', 'of', 'cool', 'things', 'in', 'this', 'like', 'm', '##j', 'turning', 'into', 'a', 'car', 'and', 'a', 'robot', 'and', 'the', 'whole', 'speed', 'demon', 'sequence', '.', 'also', ',', 'the', 'director', 'must', 'have', 'had', 'the', 'patience', 'of', 'a', 'saint', 'when', 'it', 'came', 'to', 'filming', 'the', 'kidd', '##y', 'bad', 'sequence', 'as', 'usually', 'directors', 'hate', 'working', 'with', 'one', 'kid', 'let', 'alone', 'a', 'whole', 'bunch', 'of', 'them', 'performing', 'a', 'complex', 'dance', 'scene', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'bottom', 'line', ',', 'this', 'movie', 'is', 'for', 'people', 'who', 'like', 'm', '##j', 'on', 'one', 'level', 'or', 'another', '(', 'which', 'i', 'think', 'is', 'most', 'people', ')', '.', 'if', 'not', ',', 'then', 'stay', 'away', '.', 'it', 'does', 'try', 'and', 'give', 'off', 'a', 'whole', '##some', 'message', 'and', 'ironically', 'm', '##j', \"'\", 's', 'best', '##est', 'buddy', 'in', 'this', 'movie', 'is', 'a', 'girl', '!', 'michael', 'jackson', 'is', 'truly', 'one', 'of', 'the', 'most', 'talented', 'people', 'ever', 'to', 'grace', 'this', 'planet', 'but', 'is', 'he', 'guilty', '?', 'well', ',', 'with', 'all', 'the', 'attention', 'i', \"'\", 've', 'gave', 'this', 'subject', '.', '.', '.', '.', 'hmm', '##m', 'well', 'i', 'don', \"'\", 't', 'know', 'because', 'people', 'can', 'be', 'different', 'behind', 'closed', 'doors', ',', 'i', 'know', 'this', 'for', 'a', 'fact', '.', 'he', 'is', 'either', 'an', 'extremely', 'nice', 'but', 'stupid', 'guy', 'or', 'one', 'of', 'the', 'most', 'sick', '##est', 'liar', '##s', '.', 'i', 'hope', 'he', 'is', 'not', 'the', 'latter', '.', '\"']\n"]}]},{"cell_type":"code","source":["#We will use only first 200 tokens to do classification (this value can be changed)\n","max_length = 200\n","tokenized_texts = [sent[:max_length] for sent in tokenized_texts]"],"metadata":{"id":"1SYpZa_8CN1X","executionInfo":{"status":"ok","timestamp":1748399786591,"user_tz":-330,"elapsed":453,"user":{"displayName":"Mukesh Kumar","userId":"00664345223903384782"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["\n","\n","for i in range(len(tokenized_texts)):\n","    sent = tokenized_texts[i]\n","    sent = ['[CLS]'] + sent + ['[SEP]']\n","    tokenized_texts[i] = sent\n","\n"],"metadata":{"id":"503MndoICKXQ","executionInfo":{"status":"ok","timestamp":1748399828451,"user_tz":-330,"elapsed":82,"user":{"displayName":"Mukesh Kumar","userId":"00664345223903384782"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["print(tokenized_texts[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UAR0IX3wCPQ3","executionInfo":{"status":"ok","timestamp":1748399830971,"user_tz":-330,"elapsed":12,"user":{"displayName":"Mukesh Kumar","userId":"00664345223903384782"}},"outputId":"e407c5d3-8162-41e1-92f3-a18b8a7b459f"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["['[CLS]', '\"', 'with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'm', '##j', 'i', \"'\", 've', 'started', 'listening', 'to', 'his', 'music', ',', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', ',', 'watched', 'the', 'wi', '##z', 'and', 'watched', 'moon', '##walker', 'again', '.', 'maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent', '.', 'moon', '##walker', 'is', 'part', 'biography', ',', 'part', 'feature', 'film', 'which', 'i', 'remember', 'going', 'to', 'see', 'at', 'the', 'cinema', 'when', 'it', 'was', 'originally', 'released', '.', 'some', 'of', 'it', 'has', 'subtle', 'messages', 'about', 'm', '##j', \"'\", 's', 'feeling', 'towards', 'the', 'press', 'and', 'also', 'the', 'obvious', 'message', 'of', 'drugs', 'are', 'bad', 'm', \"'\", 'kay', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'visually', 'impressive', 'but', 'of', 'course', 'this', 'is', 'all', 'about', 'michael', 'jackson', 'so', 'unless', 'you', 'remotely', 'like', 'm', '##j', 'in', 'anyway', 'then', 'you', 'are', 'going', 'to', 'hate', 'this', 'and', 'find', 'it', 'boring', '.', 'some', 'may', 'call', 'm', '##j', 'an', 'ego', '##tist', 'for', 'consent', '##ing', 'to', 'the', 'making', 'of', 'this', 'movie', 'but', 'm', '##j', 'and', 'most', 'of', 'his', 'fans', 'would', 'say', 'that', 'he', 'made', 'it', 'for', 'the', '[SEP]']\n"]}]},{"cell_type":"code","source":["#Convert tokens into IDs\n","input_ids = [tokenizer.convert_tokens_to_ids(sent) for sent in tokenized_texts]\n"],"metadata":{"id":"KgjXS0p0DKxJ","executionInfo":{"status":"ok","timestamp":1748399859473,"user_tz":-330,"elapsed":14069,"user":{"displayName":"Mukesh Kumar","userId":"00664345223903384782"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["print(input_ids[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DdtzaoqoDOUY","executionInfo":{"status":"ok","timestamp":1748399859490,"user_tz":-330,"elapsed":9,"user":{"displayName":"Mukesh Kumar","userId":"00664345223903384782"}},"outputId":"71d3001f-1785-40a4-9c44-21af9e0df79e"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["[101, 1000, 2007, 2035, 2023, 4933, 2183, 2091, 2012, 1996, 2617, 2007, 1049, 3501, 1045, 1005, 2310, 2318, 5962, 2000, 2010, 2189, 1010, 3666, 1996, 5976, 4516, 2182, 1998, 2045, 1010, 3427, 1996, 15536, 2480, 1998, 3427, 4231, 26965, 2153, 1012, 2672, 1045, 2074, 2215, 2000, 2131, 1037, 3056, 12369, 2046, 2023, 3124, 2040, 1045, 2245, 2001, 2428, 4658, 1999, 1996, 27690, 2074, 2000, 2672, 2191, 2039, 2026, 2568, 3251, 2002, 2003, 5905, 2030, 7036, 1012, 4231, 26965, 2003, 2112, 8308, 1010, 2112, 3444, 2143, 2029, 1045, 3342, 2183, 2000, 2156, 2012, 1996, 5988, 2043, 2009, 2001, 2761, 2207, 1012, 2070, 1997, 2009, 2038, 11259, 7696, 2055, 1049, 3501, 1005, 1055, 3110, 2875, 1996, 2811, 1998, 2036, 1996, 5793, 4471, 1997, 5850, 2024, 2919, 1049, 1005, 10905, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 17453, 8052, 2021, 1997, 2607, 2023, 2003, 2035, 2055, 2745, 4027, 2061, 4983, 2017, 19512, 2066, 1049, 3501, 1999, 4312, 2059, 2017, 2024, 2183, 2000, 5223, 2023, 1998, 2424, 2009, 11771, 1012, 2070, 2089, 2655, 1049, 3501, 2019, 13059, 16774, 2005, 9619, 2075, 2000, 1996, 2437, 1997, 2023, 3185, 2021, 1049, 3501, 1998, 2087, 1997, 2010, 4599, 2052, 2360, 2008, 2002, 2081, 2009, 2005, 1996, 102]\n"]}]},{"cell_type":"code","source":["\n","#Pad our tokens which might be less than max_length size\n","input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, maxlen=max_length+2, truncating='post', padding='post')\n"],"metadata":{"id":"vOl-WY9oDI4H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Split data between training and test"],"metadata":{"id":"FEyubOEnDR8f"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n"],"metadata":{"id":"O0GXK3iSDQZn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#80% data will be used for training while 20% will be used for test\n","trainX, testX, trainY, testY = train_test_split(input_ids, labels, test_size=0.2, random_state=12345)"],"metadata":{"id":"h_C9g7yEDMZ_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Create Attention masks : Attention masks are useful to ignore padding tokens. Mask value will be set to 0 for padding tokens and 1 for actual tokens. We will create mask both for training and test data\n","\n","\n","Sources\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"LYZC17NZDoao"}},{"cell_type":"code","source":["# Create attention masks for training\n","train_attn_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in trainX:\n","    seq_mask = [float(i>0) for i in seq]\n","    train_attn_masks.append(seq_mask)\n"],"metadata":{"id":"U2Xe4YtIDqXX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Create attention masks for Test\n","test_attn_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in testX:\n","    seq_mask = [float(i>0) for i in seq]\n","    test_attn_masks.append(seq_mask)"],"metadata":{"id":"iZeOjzI-Dt7g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Our Data is ready at this point\n"],"metadata":{"id":"zGuY7LDPDyE_"}},{"cell_type":"code","source":[],"metadata":{"id":"ipFoFKzCD0TL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## BUILD MODEL"],"metadata":{"id":"c4XXDPcaD1N3"}},{"cell_type":"code","source":["#Load Pre-trained Bert Model with a Binary Classification layer at the top.\n","#Huggingface library provides TFBertForSequenceClassification for the same\n","model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n","\n","# model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2-rDXt0KD2X7","executionInfo":{"status":"ok","timestamp":1748357944801,"user_tz":-330,"elapsed":7284,"user":{"displayName":"Mukesh Kumar","userId":"00664345223903384782"}},"outputId":"b74b5cc7-1b7f-4a7d-cae9-b12072933cc5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.51.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n","Loaded 109,482,240 parameters in the TF 2.0 model.\n","All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n","\n","Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule\n","optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n","model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"],"metadata":{"id":"7CIrEat_EOcX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4zejEitBENrv","executionInfo":{"status":"ok","timestamp":1748357944904,"user_tz":-330,"elapsed":93,"user":{"displayName":"Mukesh Kumar","userId":"00664345223903384782"}},"outputId":"be85414c-9bc6-49e7-cdde-43fde95abdb6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"tf_bert_for_sequence_classification\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," bert (TFBertMainLayer)      multiple                  109482240 \n","                                                                 \n"," dropout_37 (Dropout)        multiple                  0 (unused)\n","                                                                 \n"," classifier (Dense)          multiple                  1538      \n","                                                                 \n","=================================================================\n","Total params: 109483778 (417.65 MB)\n","Trainable params: 109483778 (417.65 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["## TRAIN MODEL"],"metadata":{"id":"9a2AzMlTGD3P"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"riDmJZ0BGHq3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_x_data = {'input_ids': np.array(trainX), 'attention_mask': np.array(train_attn_masks)}\n","test_x_data = {'input_ids': np.array(testX), 'attention_mask': np.array(test_attn_masks)}"],"metadata":{"id":"jXH4hOqmGE8Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.fit(train_x_data, trainY, validation_data=(test_x_data, testY), batch_size=16, epochs=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321},"id":"KE1thbN2GGwG","outputId":"8a827b93-f50a-46b5-e969-58507425dd76","executionInfo":{"status":"error","timestamp":1748367122927,"user_tz":-330,"elapsed":398869,"user":{"displayName":"Mukesh Kumar","userId":"00664345223903384782"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n"," 254/1250 [=====>........................] - ETA: 9:54:33 - loss: 0.3817 - accuracy: 0.8209"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-6c14ee942297>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_batch_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1802\u001b[0m                         ):\n\u001b[1;32m   1803\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1804\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1805\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1806\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    870\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["\"\"\"## PREDICT ON TEST DATA\"\"\"\n","\n","# Get the raw predictions (logits)\n","predictions = model.predict(test_x_data)\n","\n","# Convert logits to predicted labels (0 or 1)\n","predicted_labels = np.argmax(predictions.logits, axis=1)\n","\n","# Evaluate accuracy manually\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","print(\"Test Accuracy:\", accuracy_score(testY, predicted_labels))\n","print(\"\\nClassification Report:\\n\", classification_report(testY, predicted_labels))\n","print(\"\\nConfusion Matrix:\\n\", confusion_matrix(testY, predicted_labels))\n"],"metadata":{"id":"apX10-mVvtRB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Show few predictions vs actual\n","for i in range(5):\n","    print(f\"\\nReview: {tokenizer.decode(testX[i], skip_special_tokens=True)}\")\n","    print(f\"Actual Sentiment: {'Positive' if testY[i]==1 else 'Negative'}\")\n","    print(f\"Predicted Sentiment: {'Positive' if predicted_labels[i]==1 else 'Negative'}\")\n"],"metadata":{"id":"wi2_wptFvuop"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict_sentiment(text, tokenizer, model, max_length=200):\n","    # Tokenize with special tokens [CLS] and [SEP]\n","    tokens = tokenizer.tokenize(text)\n","    tokens = tokens[:max_length]  # Truncate\n","    tokens = ['[CLS]'] + tokens + ['[SEP]']\n","\n","    # Convert to IDs\n","    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","    # Pad to max_length+2 to match training input\n","    input_ids = tf.keras.preprocessing.sequence.pad_sequences([input_ids], maxlen=max_length+2, padding='post')\n","\n","    # Create attention mask\n","    attn_mask = [[float(i > 0) for i in input_ids[0]]]\n","\n","    # Prepare input dict\n","    inputs = {\n","        'input_ids': tf.convert_to_tensor(input_ids),\n","        'attention_mask': tf.convert_to_tensor(attn_mask)\n","    }\n","\n","    # Predict\n","    outputs = model.predict(inputs)\n","    pred_label = np.argmax(outputs.logits, axis=1)[0]\n","\n","    sentiment = \"Positive\" if pred_label == 1 else \"Negative\"\n","    print(f\"Review: {text}\\nPredicted Sentiment: {sentiment}\")\n","    return sentiment\n"],"metadata":{"id":"wqJmDPm-wEkA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predict_sentiment(\"I loved the movie very much\", tokenizer, model)\n"],"metadata":{"id":"Y5APpENdwGXA"},"execution_count":null,"outputs":[]}]}