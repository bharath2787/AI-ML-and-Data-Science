{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP5sVoGyjsfX47wQFxJcQxK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qfJlRmY7bk_k"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["Ordinary Least Squares (OLS) is not scalable in linear regression for large datasets or high-dimensional problems due to the computational and memory inefficiencies of its core operations. Here's why:\n","\n","### 1. **Matrix Inversion Complexity**\n","   - In OLS, the coefficients are calculated using the formula:  \n","     $[\n","     \\hat{\\beta} = (X^T X)^{-1} X^T y\n","     ]$\n","   - The step $((X^T X)^{-1})$ involves inverting a matrix of size $(n \\times n)$, where \\(n\\) is the number of features (predictors).\n","   - The time complexity for matrix inversion is \\(O(n^3)\\), making it computationally expensive as the number of features grows.\n","\n","### 2. **Memory Usage**\n","   - The matrix $(X^T X)$ has size $(n \\times n)$, and storing it in memory becomes impractical for high-dimensional datasets (large \\(n\\)).\n","   - Similarly, storing the input matrix \\(X\\) itself can be infeasible for datasets with many observations (\\(m\\)) and features (\\(n\\)).\n","\n","### 3. **Numerical Stability**\n","   - The matrix inversion process can be numerically unstable, especially if \\(X^T X\\) is close to singular (i.e., not invertible or ill-conditioned). This instability can lead to inaccurate results.\n","\n","### 4. **Scaling with Large Data**\n","   - When the number of observations (\\(m\\)) is very large, computing $(X^T X)$ requires $(O(mn^2))$ operations. This scales poorly with \\(m\\), making it unsuitable for large datasets.\n","\n","### Alternatives for Scalability\n","To address these limitations, more scalable approaches are used for linear regression:\n","\n","1. **Gradient Descent**  \n","   - Iterative optimization algorithms like Stochastic Gradient Descent (SGD) compute updates for the coefficients in smaller batches of data. This avoids the need for matrix inversion and reduces memory requirements.\n","   - Time complexity per iteration: $(O(mn)$) for full-batch or \\(O(n)\\) for mini-batch SGD.\n","\n","2. **Stochastic Methods**\n","   - Methods like coordinate descent focus on updating one parameter at a time, reducing computational overhead.\n","\n","3. **Distributed Computing**\n","   - For massive datasets, distributed frameworks like Apache Spark implement linear regression using iterative methods across clusters.\n","\n","4. **Regularization Techniques**\n","   - When using regularization (e.g., Ridge or Lasso), the design of the problem inherently changes, sometimes allowing better scalability and handling of ill-conditioning.\n","\n","OLS remains useful for small to medium-sized datasets due to its simplicity and exact solutions, but its scalability issues necessitate alternative methods for large-scale applications."],"metadata":{"id":"DXY1m5O6bqnZ"}},{"cell_type":"code","source":[],"metadata":{"id":"eD3-lZxobq-H"},"execution_count":null,"outputs":[]}]}