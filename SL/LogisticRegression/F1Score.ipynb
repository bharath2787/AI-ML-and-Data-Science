{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNqjb1ylWk+TYf2f1gl4JU+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"JsukaxDY26DO"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["Let's explore how different values of Precision and Recall affect the F1 score by calculating it for various scenarios.\n","\n","### Scenario 1: High Precision, Low Recall\n","- **Precision = 0.9**\n","- **Recall = 0.5**\n","\n","**F1 Score Calculation:**\n","$[\n","\\text{F1 Score} = 2 \\times \\frac{0.9 \\times 0.5}{0.9 + 0.5} = 2 \\times \\frac{0.45}{1.4} \\approx 0.64\n","]$\n","- **Interpretation**: Here, the precision is high, indicating that when the model predicts a positive, itâ€™s usually correct. However, recall is low, meaning the model misses a lot of actual positives. The F1 score is moderate because the low recall brings it down.\n","\n","### Scenario 2: Low Precision, High Recall\n","- **Precision = 0.5**\n","- **Recall = 0.9**\n","\n","**F1 Score Calculation:**\n","$[\n","\\text{F1 Score} = 2 \\times \\frac{0.5 \\times 0.9}{0.5 + 0.9} = 2 \\times \\frac{0.45}{1.4} \\approx 0.64\n","]$\n","- **Interpretation**: In this case, the model captures most of the positives (high recall) but also produces many false positives (low precision). The F1 score is still moderate, similar to the first scenario, highlighting how it balances both metrics.\n","\n","### Scenario 3: Balanced Precision and Recall\n","- **Precision = 0.7**\n","- **Recall = 0.7**\n","\n","**F1 Score Calculation:**\n","$[\n","\\text{F1 Score} = 2 \\times \\frac{0.7 \\times 0.7}{0.7 + 0.7} = 2 \\times \\frac{0.49}{1.4} = 0.7\n","]$\n","- **Interpretation**: Here, both precision and recall are balanced, leading to a higher F1 score. The model is performing well on both metrics.\n","\n","### Scenario 4: Perfect Precision and Recall\n","- **Precision = 1.0**\n","- **Recall = 1.0**\n","\n","**F1 Score Calculation:**\n","$[\n","\\text{F1 Score} = 2 \\times \\frac{1.0 \\times 1.0}{1.0 + 1.0} = 2 \\times \\frac{1.0}{2.0} = 1.0\n","]$\n","- **Interpretation**: When both precision and recall are perfect, the F1 score is also perfect, reflecting an ideal model.\n","\n","### Scenario 5: Low Precision and Low Recall\n","- **Precision = 0.3**\n","- **Recall = 0.3**\n","\n","**F1 Score Calculation:**\n","$[\n","\\text{F1 Score} = 2 \\times \\frac{0.3 \\times 0.3}{0.3 + 0.3} = 2 \\times \\frac{0.09}{0.6} = 0.3\n","]$\n","- **Interpretation**: Both precision and recall are low, resulting in a low F1 score, indicating poor model performance.\n","\n","### Summary of Examples:\n","- **F1 Score is highest** when both Precision and Recall are high.\n","- **F1 Score decreases** significantly when either Precision or Recall is low, even if the other metric is high.\n","- **Balanced Precision and Recall** lead to a balanced F1 score.\n","- **Perfect Precision and Recall** yield the maximum possible F1 score.\n","\n","These examples show that the F1 score is a useful metric to summarize the trade-off between precision and recall. It provides a single measure of performance that considers both the number of false positives and false negatives, especially important in cases of imbalanced datasets."],"metadata":{"id":"bwei5HEC3Kl_"}},{"cell_type":"code","source":[],"metadata":{"id":"Ri44QkAw3LTN"},"execution_count":null,"outputs":[]}]}