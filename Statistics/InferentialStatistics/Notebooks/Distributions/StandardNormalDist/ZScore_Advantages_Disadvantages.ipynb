{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOaKLFrxQkoOcyUWSrnKujx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MQ9eE-vKkXiH"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["### **Z-Score Transformation: Advantages & Disadvantages**  \n","\n","#### ✅ **Advantages**  \n","1. **Standardization** – Converts data to a common scale with **mean = 0** and **standard deviation = 1**, making features comparable.  \n","2. **Works Well with Normal Distribution** – Useful for normally distributed data, where many statistical and ML algorithms assume standard normality.  \n","3. **Helps in Outlier Detection** – Z-scores highlight outliers as values significantly far from 0 (e.g., |Z| > 3).  \n","4. **Essential for Distance-Based Algorithms** – Improves performance in models like **KNN, SVM, and PCA**, which rely on feature magnitudes.  \n","5. **Retains Relative Variance Within Features** – While absolute distances between features are lost, the spread within each feature remains.  \n","\n","#### ❌ **Disadvantages**  \n","1. **Loss of Absolute Scale** – Original units and absolute distances between features are lost, making it hard to interpret raw values.  \n","2. **Not Ideal for Non-Normal Data** – If data is heavily skewed or non-Gaussian, Z-score transformation might not be the best choice.  \n","3. **Feature Distance is Lost** – The relative difference between features (e.g., Feature_A and Feature_B) is removed.  \n","4. **Sensitive to Outliers** – Since it uses mean and standard deviation, extreme values can significantly affect the transformation.  \n","5. **Might Not Be Reversible** – If you need to restore original values later, Z-score transformation makes it harder compared to min-max scaling.  \n","\n","### **When to Use & When to Avoid?**  \n","| **Use Z-Score When...** | **Avoid Z-Score When...** |  \n","|--------------------------|----------------------------|  \n","| Features have different scales but should be **equally weighted** (e.g., ML models) | You need to **preserve the original distances** between features |  \n","| Data follows a **normal distribution** | Data is **highly skewed** or non-Gaussian |  \n","| Distance-based algorithms are used (KNN, PCA, SVM) | Raw feature values are **important for interpretation** |  \n","| You need to detect **outliers easily** | The dataset is **small**, making mean & std dev unstable |  \n"],"metadata":{"id":"UWyyHtQekc_d"}},{"cell_type":"code","source":[],"metadata":{"id":"bxUNFLBtkguU"},"execution_count":null,"outputs":[]}]}