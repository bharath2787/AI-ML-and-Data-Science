{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPkjzqBaHpyKbdfpKqDzpf9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"D0FiAshRpghu"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["\n","---\n","\n","###  **Curse of Dimensionality — Explained **\n","\n","As the **number of features (dimensions)** increases, the **data becomes sparse** in the high-dimensional space.\n","\n","This leads to problems like:\n","\n","- **Sparse data**: Even a large dataset starts to feel empty in higher dimensions, because data points spread out and rarely cluster.\n","-  **Hard to learn patterns**: ML models rely on nearby data points to learn relationships. In high dimensions, **everything feels far apart**, so it's hard to capture meaningful patterns.\n","-  **Overfitting risk**: The model might latch onto noise instead of actual trends because there aren’t enough nearby points to learn generalizable features.\n","-  **Computationally expensive**: More dimensions = more calculations, more parameters = higher training time and complexity.\n","\n","---\n","\n","###  Intuition\n","\n","In 1D, you can cover an interval [0, 1] with 10 points.\n","\n","In 10D, to get the **same coverage density**, you'd need $(10^{10})$ points — totally impractical!\n","\n","---\n","\n","###  Why PCA Helps\n","\n","By projecting the data onto the directions with the **most variance**, PCA helps reduce the number of dimensions while **retaining the most important information**, helping mitigate the curse.\n","\n","---\n","\n"],"metadata":{"id":"qrF7I0n5pg_S"}},{"cell_type":"code","source":[],"metadata":{"id":"u_ZBFq5wpsgj"},"execution_count":null,"outputs":[]}]}