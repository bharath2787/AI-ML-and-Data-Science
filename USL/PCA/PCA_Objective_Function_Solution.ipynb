{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM7Rn8lULUlh/YoG7BEEd0q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0hHopZ5pebZm"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["To connect the dots between solving the objective function and concluding that the vector we were looking for is an **eigenvector**, we need to follow a series of logical steps. Let's walk through the process carefully:\n","\n","### Step 1: **PCA Objective**\n","\n","The goal of Principal Component Analysis (PCA) is to find the **direction (or vector)** in the feature space that maximizes the variance of the projected data points. In mathematical terms, we want to find the vector $( \\mathbf{w} )$ (also called the principal component direction) that maximizes the variance in the data.\n","\n","#### Objective:\n","Maximize the variance along a direction $( \\mathbf{w} )$ subject to the constraint that $( \\mathbf{w} )$ is a unit vector (to avoid scaling issues). This can be framed as the following optimization problem:\n","\n","$[\n","\\max_{\\mathbf{w}} \\quad \\mathbf{w}^\\top S \\mathbf{w} \\quad \\text{subject to} \\quad \\mathbf{w}^\\top \\mathbf{w} = 1\n","]$\n","\n","Where:\n","- $( S )$ is the covariance matrix of the dataset.\n","- $( \\mathbf{w} )$ is the direction we are looking for (the principal component).\n","- $( \\mathbf{w}^\\top S \\mathbf{w} )$ represents the variance along the direction $( \\mathbf{w} )$.\n","\n","### Step 2: **Using Lagrange Multipliers**\n","\n","To solve this constrained optimization problem, we use **Lagrange multipliers**. The Lagrangian function for this optimization is:\n","\n","$[\n","\\mathcal{L}(\\mathbf{w}, \\lambda) = \\mathbf{w}^\\top S \\mathbf{w} - \\lambda (\\mathbf{w}^\\top \\mathbf{w} - 1)\n","]$\n","\n","Where $( \\lambda )$ is the Lagrange multiplier associated with the constraint $( \\mathbf{w}^\\top \\mathbf{w} = 1 )$.\n","\n","### Step 3: **Taking the Derivative**\n","\n","Next, we take the derivative of the Lagrangian function with respect to $( \\mathbf{w} )$ and set it equal to zero to find the extremum:\n","\n","$[\n","\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}} = 2S\\mathbf{w} - 2\\lambda \\mathbf{w} = 0\n","]$\n","\n","Simplifying:\n","\n","$[\n","S\\mathbf{w} = \\lambda \\mathbf{w}\n","]$\n","\n","This is an **eigenvalue equation**.\n","\n","### Step 4: **Eigenvalue Equation**\n","\n","The equation $( S\\mathbf{w} = \\lambda \\mathbf{w} )$ is the **standard eigenvalue equation**:\n","\n","$[\n","A \\mathbf{v} = \\lambda \\mathbf{v}\n","]$\n","\n","Where:\n","- $( A = S )$ (the covariance matrix).\n","- $( \\mathbf{v} = \\mathbf{w} )$ (the vector we are solving for).\n","- $( \\lambda )$ is the eigenvalue (which corresponds to the variance along that direction).\n","\n","### Step 5: **Connecting to Eigenvectors and Eigenvalues**\n","\n","From the eigenvalue equation $( S\\mathbf{w} = \\lambda \\mathbf{w} )$, we can immediately recognize the following:\n","\n","- The vector $( \\mathbf{w} )$ is an **eigenvector** of the covariance matrix $( S )$, because it satisfies the equation $( S\\mathbf{w} = \\lambda \\mathbf{w} )$.\n","- The scalar $( \\lambda )$ is the corresponding **eigenvalue**, which in the context of PCA represents the variance captured by that eigenvector (or principal component).\n","\n","### Step 6: **Conclusion**\n","\n","Thus, we have derived that:\n","- The vector $( \\mathbf{w} )$ that maximizes the variance in the data (the principal component direction) is an **eigenvector** of the covariance matrix $( S )$.\n","- The associated eigenvalue $( \\lambda )$ tells us the **variance** (or amount of information) captured along that direction.\n","\n","### Final Thoughts\n","\n","In summary, the process works like this:\n","- The optimization problem for PCA seeks the direction in the feature space that maximizes the variance of the data.\n","- By solving the optimization problem with Lagrange multipliers, we arrive at the **eigenvalue equation**.\n","- The solutions to this equation (i.e., the vectors $( \\mathbf{w} )$) are the **eigenvectors** of the covariance matrix, and the corresponding values $( \\lambda )$ are the **eigenvalues**.\n","- These eigenvectors correspond to the **principal components** of the data, and the eigenvalues tell us how much variance each principal component captures.\n","\n"],"metadata":{"id":"aUrUH6oPeb-M"}},{"cell_type":"code","source":[],"metadata":{"id":"_4eWmrz5edcn"},"execution_count":null,"outputs":[]}]}